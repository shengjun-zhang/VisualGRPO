# Copyright (c) [2025] [FastVideo Team]
# Copyright (c) [2025] [ByteDance Ltd. and/or its affiliates.]
# SPDX-License-Identifier: [Apache License 2.0] 
#
# This file has been modified by [ByteDance Ltd. and/or its affiliates.] in 2025.
#
# Original file was released under [Apache License 2.0], with the full license text
# available at [https://github.com/hao-ai-lab/FastVideo/blob/main/LICENSE].
#
# This modified file is released under the same license.
#
# Modified for Tree-based GRPO with split rollout strategy
# 
# Latest modifications (support multi-batch training):
# - Added use_group logic support, consistent with original train_grpo_flux.py
# - Fixed hardcoded batch_size=1 issue, now supports train_batch_size > 1
# - Support parameters: --train_batch_size 2 --train_sp_batch_size 2 --use_group
# - When using use_group, each prompt generates num_generations samples for comparison

import argparse
import math
import os
from pathlib import Path
from fastvideo.utils.parallel_states import (
    initialize_sequence_parallel_state,
    destroy_sequence_parallel_group,
    get_sequence_parallel_state,
    nccl_info,
)
from fastvideo.utils.communications_flux import sp_parallel_dataloader_wrapper
from fastvideo.utils.validation import log_validation
import time
from torch.utils.data import DataLoader
import torch
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

from torch.utils.data.distributed import DistributedSampler
from fastvideo.utils.dataset_utils import LengthGroupedSampler
import wandb
from accelerate.utils import set_seed
from tqdm.auto import tqdm
from fastvideo.utils.fsdp_util import get_dit_fsdp_kwargs, apply_fsdp_checkpointing
from fastvideo.utils.load import load_transformer
from diffusers.optimization import get_scheduler
from diffusers.utils import check_min_version
from fastvideo.dataset.latent_flux_rl_datasets import LatentDataset, latent_collate_function
import torch.distributed as dist
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from fastvideo.utils.checkpoint import (
    save_checkpoint,
    save_lora_checkpoint,
)
from fastvideo.utils.logging_ import main_print
import cv2
from diffusers.image_processor import VaeImageProcessor
from datetime import datetime
import yaml
import subprocess
import json

def generate_experiment_name(args):
    """Generate unique experiment name based on time and key parameters"""
    now = datetime.now().strftime("%Y-%m-%d-%H-%M")
    name = f"BranchGRPO_{now}"
    
    # Add key tree parameters to distinguish experiments
    if hasattr(args, 'tree_split_points') and args.tree_split_points:
        # If using custom split points, show number of points
        points = args.tree_split_points.split(',')
        name += f"_sp{len(points)}pts"
    elif hasattr(args, 'tree_split_rounds'):
        name += f"_r{args.tree_split_rounds}"
    
    if hasattr(args, 'tree_split_noise_scale'):
        name += f"_ns{args.tree_split_noise_scale}"
    if hasattr(args, 'learning_rate'):
        name += f"_lr{args.learning_rate}"
    if hasattr(args, 'clip_range'):
        name += f"_clip{args.clip_range}"
    if hasattr(args, 'tree_split_points'):
        name += f"_sp{args.tree_split_points}"
    
    # Add depth pruning identifier
    if hasattr(args, 'depth_pruning') and args.depth_pruning:
        depths = args.depth_pruning.split(',')
        name += f"_dp{args.depth_pruning}"  # e.g., _dp5d means pruning 5 depths
    
        if hasattr(args, 'depth_pruning_slide') and args.depth_pruning_slide:
            name += f"_dp_slide"
      
    # Add width pruning identifier
    if hasattr(args, 'width_pruning_mode') and args.width_pruning_mode is not None and args.width_pruning_mode > 0:
        ratio = getattr(args, 'width_pruning_ratio', 0.5)
        name += f"_wp{args.width_pruning_mode}"  # e.g., _wp1 means mode 1
        

    if hasattr(args, 'tree_prob_weighted') and args.tree_prob_weighted:
        name += f"_tpw"

    # Mixed ODE/SDE sliding window marker
    if hasattr(args, 'mix_ode_sde_tree') and args.mix_ode_sde_tree:
        win = getattr(args, 'mix_sde_window_size', 4)
        name += f"_mixwin{win}"

  
    return name

def save_experiment_config(args, exp_name, rank):
    """Save experiment configuration and Git information"""
    if rank > 0:
        return  # Only save in main process
    
    # Create experiment directory
    exp_log_dir = f"log/{exp_name}"
    os.makedirs(exp_log_dir, exist_ok=True)
    
    # Save configuration as YAML
    config_path = f"{exp_log_dir}/config.yaml"
    config_dict = vars(args)
    with open(config_path, 'w') as f:
        yaml.dump(config_dict, f, default_flow_style=False, sort_keys=True)
    
    # Save Git information
    git_info_path = f"{exp_log_dir}/git_info.txt"
    try:
        # Get Git commit hash
        commit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'], 
                                            stderr=subprocess.DEVNULL).decode().strip()
        # Get branch name
        branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], 
                                       stderr=subprocess.DEVNULL).decode().strip()
        # Get working directory status
        status = subprocess.check_output(['git', 'status', '--porcelain'], 
                                       stderr=subprocess.DEVNULL).decode().strip()
        
        git_info = f"Commit: {commit_hash}\nBranch: {branch}\n"
        if status:
            git_info += f"Working directory status:\n{status}\n"
        else:
            git_info += "Working directory clean\n"
            
        with open(git_info_path, 'w') as f:
            f.write(git_info)
    except (subprocess.CalledProcessError, FileNotFoundError):
        with open(git_info_path, 'w') as f:
            f.write("Git information not available\n")
    
    print(f"Experiment config saved to {exp_log_dir}")

def parse_split_points(args, total_steps):
    """Parse split points parameters"""
    if hasattr(args, 'tree_split_points') and args.tree_split_points:
        # Use custom split points
        points = [int(p.strip()) for p in args.tree_split_points.split(',')]
        # Ensure split points are within valid range
        points = [min(max(p, 0), total_steps - 1) for p in points]
        return sorted(points)
    else:
        # Use default uniform splitting
        if hasattr(args, 'tree_split_rounds') and args.tree_split_rounds > 0:
            if total_steps % args.tree_split_rounds != 0:
                print(f"Warning: total_steps ({total_steps}) is not divisible by tree_split_rounds ({args.tree_split_rounds})")
            split_interval = total_steps // args.tree_split_rounds
            points = [i * split_interval for i in range(args.tree_split_rounds)]
            return [min(p, total_steps - 1) for p in points]
        else:
            return []

# Will error if the minimal version of diffusers is not installed. Remove at your own risks.
check_min_version("0.31.0")
import time
from collections import deque
import numpy as np
from einops import rearrange
import torch.distributed as dist
from torch.nn import functional as F
from typing import List, Dict, Tuple
from PIL import Image
from diffusers import FluxTransformer2DModel, AutoencoderKL
import random


class TreeNode:
    """Tree node for tracking branching rollout tree structure"""
    def __init__(self, node_id: str, latent: torch.Tensor, parent=None, step: int = 0, batch_idx: int = 0):
        self.node_id = node_id
        self.latent = latent
        self.parent = parent
        self.children = []
        self.step = step
        self.batch_idx = batch_idx  # Add batch_idx field to record which batch sample this belongs to
        self.log_prob = None  # log_prob from parent to this node
        self.reward = None    # Node reward value (leaf nodes have actual reward, internal nodes have aggregated reward)
        self.advantage = None # Node advantage value
        self.depth = 0 if parent is None else parent.depth + 1  # Node depth
        self.is_sde_edge = None  # Whether the edge from parent to this node is SDE generated (for training filtering)
        
    def add_child(self, child):
        self.children.append(child)
        child.depth = self.depth + 1  # Update child node depth
        
    def is_leaf(self):
        return len(self.children) == 0
        
    def get_path_from_root(self):
        """Get path from root node to current node"""
        path = []
        current = self
        while current is not None:
            path.append(current)
            current = current.parent
        return list(reversed(path))
    
    def get_all_leaf_descendants(self):
        """Get all leaf descendants of current node"""
        if self.is_leaf():
            return [self]
        
        leaves = []
        for child in self.children:
            leaves.extend(child.get_all_leaf_descendants())
        return leaves


def sd3_time_shift(shift, t):
    return (shift * t) / (1 + (shift - 1) * t)


def flux_step_with_split(
    model_output: torch.Tensor,
    latents: torch.Tensor,
    eta: float,
    sigmas: torch.Tensor,
    index: int,
    prev_sample: torch.Tensor,
    grpo: bool,
    sde_solver: bool,
    num_splits: int = 1,
    split_noise_scale: float = 1.0,
):
    """
    Modified flux_step that supports splitting operations
    When num_splits > 1, generates multiple different samples
    """
    sigma = sigmas[index]
    dsigma = sigmas[index + 1] - sigma
    prev_sample_mean = latents + dsigma * model_output

    pred_original_sample = latents - sigma * model_output

    delta_t = sigma - sigmas[index + 1]
    std_dev_t = eta * math.sqrt(delta_t)
    
    # Numerical stability protection: prevent std_dev_t from being too small, use device-consistent tensor
    std_dev_t = torch.clamp(torch.as_tensor(std_dev_t, device=latents.device, dtype=torch.float32), min=1e-8)

    if sde_solver:
        score_estimate = -(latents-pred_original_sample*(1 - sigma))/sigma**2
        log_term = -0.5 * eta**2 * score_estimate
        prev_sample_mean = prev_sample_mean + log_term * dsigma

    if grpo:
        if prev_sample is None:
            # Splitting strategy: use independent randomness to create diversity without changing global RNG
            if num_splits > 1:
                split_samples = []
                log_probs = []
                
                for i in range(num_splits):
                    # Use split_noise_scale to control noise intensity without changing global random seed
                    noise = torch.randn_like(prev_sample_mean) * split_noise_scale
                    sample = prev_sample_mean + noise * std_dev_t
                    split_samples.append(sample)
                    
                    # log_prob calculation
                    two_pi = torch.as_tensor(2 * math.pi, device=prev_sample_mean.device, dtype=torch.float32)
                    log_prob = (
                        -((sample.detach().to(torch.float32) - prev_sample_mean.to(torch.float32)) ** 2) / (2 * (std_dev_t**2))
                    ) - torch.log(std_dev_t) - 0.5 * torch.log(two_pi)
                    log_prob = log_prob.mean(dim=tuple(range(1, log_prob.ndim)))
                    log_probs.append(log_prob)
                
                # Collect results from all branches
                prev_sample = torch.cat(split_samples, dim=0)
                log_prob = torch.cat(log_probs, dim=0)
                # pred_original_sample is the same for all branches, expand with correct dimensions
                if pred_original_sample.dim() == 3:  # [B, num_patches, channels]
                    pred_original_sample = pred_original_sample.repeat(num_splits, 1, 1)
                else:  # [B, C, H, W]
                    pred_original_sample = pred_original_sample.repeat(num_splits, 1, 1, 1)
                
            else:
                # Original single sample logic
                prev_sample = prev_sample_mean + torch.randn_like(prev_sample_mean) * std_dev_t 
                two_pi = torch.as_tensor(2 * math.pi, device=prev_sample_mean.device, dtype=torch.float32)
                log_prob = (
                    -((prev_sample.detach().to(torch.float32) - prev_sample_mean.to(torch.float32)) ** 2) / (2 * (std_dev_t**2))
                ) - torch.log(std_dev_t) - 0.5 * torch.log(two_pi)
                log_prob = log_prob.mean(dim=tuple(range(1, log_prob.ndim)))
        else:
            # When prev_sample is not None, calculate log_prob for given prev_sample
            two_pi = torch.as_tensor(2 * math.pi, device=prev_sample_mean.device, dtype=torch.float32)
            log_prob = (
                -((prev_sample.detach().to(torch.float32) - prev_sample_mean.to(torch.float32)) ** 2) / (2 * (std_dev_t**2))
            ) - torch.log(std_dev_t) - 0.5 * torch.log(two_pi)
            log_prob = log_prob.mean(dim=tuple(range(1, log_prob.ndim)))

    if grpo:
        return prev_sample, pred_original_sample, log_prob
    else:
        return prev_sample_mean, pred_original_sample


def assert_eq(x, y, msg=None):
    assert x == y, f"{msg or 'Assertion failed'}: {x} != {y}"


def prepare_latent_image_ids(batch_size, height, width, device, dtype):
    latent_image_ids = torch.zeros(height, width, 3)
    latent_image_ids[..., 1] = latent_image_ids[..., 1] + torch.arange(height)[:, None]
    latent_image_ids[..., 2] = latent_image_ids[..., 2] + torch.arange(width)[None, :]

    latent_image_id_height, latent_image_id_width, latent_image_id_channels = latent_image_ids.shape

    latent_image_ids = latent_image_ids.reshape(
        latent_image_id_height * latent_image_id_width, latent_image_id_channels
    )

    return latent_image_ids.to(device=device, dtype=dtype)

def pack_latents(latents, batch_size, num_channels_latents, height, width):
    latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)
    latents = latents.permute(0, 2, 4, 1, 3, 5)
    latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)

    return latents

def unpack_latents(latents, height, width, vae_scale_factor):
    batch_size, num_patches, channels = latents.shape

    # VAE applies 8x compression on images but we must also account for packing which requires
    # latent height and width to be divisible by 2.
    height = 2 * (int(height) // (vae_scale_factor * 2))
    width = 2 * (int(width) // (vae_scale_factor * 2))

    latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)
    latents = latents.permute(0, 3, 1, 4, 2, 5)

    latents = latents.reshape(batch_size, channels // (2 * 2), height, width)

    return latents


def run_tree_sample_step(
    args,
    z,
    progress_bar,
    sigma_schedule,
    transformer,
    encoder_hidden_states, 
    pooled_prompt_embeds, 
    text_ids,
    image_ids, 
    grpo_sample,
    num_final_branches=16,
    num_split_rounds=4,
    batch_offset=0,  # Add batch_offset parameter to correctly set batch_idx
):
    """
    Branching rollout sampling step - supports multi-batch
    """
    if not grpo_sample:
        raise NotImplementedError("Tree sampling only supports grpo_sample=True")
    
    total_steps = len(sigma_schedule) - 1
    
    # Use new split points parsing function
    split_points = parse_split_points(args, total_steps)
    
    # Fix: Create independent root nodes for each batch sample, but maintain unified node list processing
    batch_size = z.shape[0]
    current_nodes = []
    
    for batch_idx in range(batch_size):
        # Create independent root node for each sample
        sample_z = z[batch_idx:batch_idx+1]  # Maintain dimensions: [1, seq_len, channels]
        # Fix: Use batch_offset to ensure correct batch_idx
        actual_batch_idx = batch_offset + batch_idx
        root = TreeNode(f"root_b{actual_batch_idx}", sample_z, parent=None, step=0, batch_idx=actual_batch_idx)
        current_nodes.append(root)
    
    all_log_probs_tree = {}
    
    main_print(f"Tree sampling: {total_steps} steps, split at {split_points}")
    main_print(f"Expected final branches per sample: {2**num_split_rounds}")
    main_print(f"Batch size: {batch_size}, Initial nodes: {len(current_nodes)}")

    # This variable will be overwritten at each step, and after the last step loop, it will save the final clean image prediction
    final_pred_originals = None

    for i in progress_bar:
        new_nodes = []  # Store all new nodes for current step
        step_pred_originals = []
        should_split = i in split_points
        num_splits = 2 if should_split else 1

        if should_split:
            main_print(f"Split at step {i}: nodes={len(current_nodes)} â†’ {len(current_nodes)*2}")
        # Sliding window: SDE within window, ODE outside window (split steps always SDE)
        use_mix = getattr(args, 'mix_ode_sde_tree', False)
        window_size = int(getattr(args, 'mix_sde_window_size', 4))
        slide_interval = int(getattr(args, 'depth_pruning_slide_interval', 1))
        if use_mix:
            stride = max(1, slide_interval)
            window_start = (i // stride) * stride  # Starting position from 0, segmented by stride
            window_end = min(len(sigma_schedule) - 2, window_start + max(1, window_size) - 1)
            in_window = (i >= window_start and i <= window_end)
            if (i == 0 or should_split or (i % stride == 0)) and (dist.get_rank() % 8 == 0):
                main_print(f"[MIX] step {i}: window=({window_start},{window_end}), in_window={in_window}, split={should_split}")
        else:
            in_window = True  # When mixing not enabled, default to full SDE

        transformer.eval()
        with torch.autocast("cuda", torch.bfloat16):
            # Key fix: Process each node independently, but don't group by batch
            for node in current_nodes:
                sigma = sigma_schedule[i]
                timestep_value = int(sigma * 1000)
                timestep = torch.full([1], timestep_value, device=z.device, dtype=torch.long)
                
                # Get corresponding input parameters based on node's batch_idx
                node_batch_idx = node.batch_idx - batch_offset  # Convert to index within current batch
                sample_encoder_hidden_states = encoder_hidden_states[node_batch_idx:node_batch_idx+1]
                sample_pooled_prompt_embeds = pooled_prompt_embeds[node_batch_idx:node_batch_idx+1] 
                sample_text_ids = text_ids[node_batch_idx:node_batch_idx+1]
                sample_image_ids = image_ids  # image_ids are usually shared
                img_ids_for_node = sample_image_ids.squeeze(0) if sample_image_ids.dim() == 3 else sample_image_ids
                
                # 1. Make prediction for single node
                pred = transformer(
                    hidden_states=node.latent,  # Now ensure it's [1, seq_len, channels]
                    encoder_hidden_states=sample_encoder_hidden_states,
                    timestep=timestep/1000,
                    guidance=torch.tensor([3.5], device=z.device, dtype=torch.bfloat16),
                    txt_ids=sample_text_ids.repeat(sample_encoder_hidden_states.shape[1], 1),  # Restore original logic
                    pooled_projections=sample_pooled_prompt_embeds,
                    img_ids=img_ids_for_node,
                    joint_attention_kwargs=None,
                    return_dict=False,
                )[0]

                # 2. Step forward with prediction results for single node:
                #    - Split step: always SDE
                #    - When sliding window enabled: SDE within window, ODE outside window
                #    - When mixing not enabled: keep SDE
                if (use_mix and (not should_split) and (not in_window)):
                    sigma = sigma_schedule[i]
                    dsigma = sigma_schedule[i + 1] - sigma
                    # Only do deterministic mean update, don't inject noise; log_prob set to 0 (will be filtered during training)
                    prev_sample_mean = node.latent.to(torch.float32) + dsigma * pred
                    # Align with existing interface: return shape consistent with SDE
                    next_latents_for_node = prev_sample_mean[0:1]
                    pred_original_for_node = (node.latent.to(torch.float32) - sigma * pred)[0:1]
                    zero_lp = torch.zeros((1,), device=node.latent.device, dtype=torch.float32)
                    log_probs_for_node = zero_lp
                    edge_is_sde = False
                else:
                    next_latents_for_node, pred_original_for_node, log_probs_for_node = flux_step_with_split(
                        pred, node.latent.to(torch.float32), args.eta, 
                        sigmas=sigma_schedule, index=i, prev_sample=None, 
                        grpo=True, sde_solver=True, num_splits=num_splits, 
                        split_noise_scale=args.tree_split_noise_scale
                    )
                    edge_is_sde = True

                # 3. Create child nodes based on step results
                if should_split:
                    for split_idx in range(num_splits):
                        child_id = f"{node.node_id}_s{i}_{split_idx}"
                        child_latent = next_latents_for_node[split_idx:split_idx+1].to(torch.bfloat16)
                        child_log_prob = log_probs_for_node[split_idx:split_idx+1]
                        
                        child = TreeNode(child_id, child_latent, parent=node, step=i+1, batch_idx=node.batch_idx)
                        child.log_prob = child_log_prob
                        child.is_sde_edge = True  # Split steps are always SDE
                        node.add_child(child)
                        new_nodes.append(child)
                        
                        step_pred_originals.append(pred_original_for_node[split_idx:split_idx+1])

                        if child_id not in all_log_probs_tree:
                           all_log_probs_tree[child_id] = []
                        all_log_probs_tree[child_id].append(child_log_prob)
                else: # No split
                    child_id = f"{node.node_id}_t{i}"
                    child_latent = next_latents_for_node[0:1].to(torch.bfloat16)
                    child_log_prob = log_probs_for_node[0:1]

                    child = TreeNode(child_id, child_latent, parent=node, step=i+1, batch_idx=node.batch_idx)
                    child.log_prob = child_log_prob
                    child.is_sde_edge = edge_is_sde
                    node.add_child(child)
                    new_nodes.append(child)

                    step_pred_originals.append(pred_original_for_node)

                    if child_id not in all_log_probs_tree:
                       all_log_probs_tree[child_id] = []
                    all_log_probs_tree[child_id].append(child_log_prob)

        # Key fix: Directly replace current_nodes, just like bs1 version
        current_nodes = new_nodes
        
        # Collect all clean image predictions generated in current step
        final_pred_originals = torch.cat(step_pred_originals, dim=0)

        if i == 0 or should_split:
            # Count nodes for each batch
            batch_node_counts = {}
            for node in current_nodes:
                batch_idx = node.batch_idx
                batch_node_counts[batch_idx] = batch_node_counts.get(batch_idx, 0) + 1
            main_print(f"Step {i}: {len(current_nodes)} total nodes, per batch: {batch_node_counts}")
            
    # After loop ends, current_nodes are leaf nodes
    leaf_nodes = current_nodes
    final_latents = torch.cat([node.latent for node in leaf_nodes], dim=0)
    
    # Build path log_probs for all leaf nodes
    path_log_probs = []
    for leaf in leaf_nodes:
        path = leaf.get_path_from_root()
        total_log_prob = torch.zeros_like(leaf.log_prob if leaf.log_prob is not None else torch.tensor(0.0))
        for node in path[1:]:  # Skip root node
            if node.log_prob is not None:
                total_log_prob += node.log_prob
        path_log_probs.append(total_log_prob)
    
    all_path_log_probs = torch.cat(path_log_probs, dim=0) if path_log_probs else torch.tensor([])
    
    # Return final noisy latent, clean image predictions, log_probs and tree structure
    return final_latents, final_pred_originals, all_path_log_probs, leaf_nodes, all_log_probs_tree


def compute_node_rewards_from_leaves(
    root_node: TreeNode,
    leaf_rewards: torch.Tensor,
    leaf_nodes: List[TreeNode],
    use_prob_weighted: bool = False,
):
    """
    Compute reward values for all nodes from bottom up
    Leaf nodes: use actual reward
    Internal nodes: use average reward of all leaf descendants
    """
    # 1. Assign actual rewards to leaf nodes
    for i, leaf_node in enumerate(leaf_nodes):
        leaf_node.reward = leaf_rewards[i]

    # 1.1 Only print path probability statistics when probability weighting is enabled (sum of log_prob from root to leaf then exp)
    if use_prob_weighted:
        try:
            path_probs = []
            for idx, leaf_node in enumerate(leaf_nodes):
                path = leaf_node.get_path_from_root()
                total_log_prob = None
                for node in path[1:]:  # Skip root node
                    if node.log_prob is not None:
                        lp = node.log_prob.squeeze().to(torch.float32)
                        total_log_prob = lp if total_log_prob is None else (total_log_prob + lp)
                if total_log_prob is not None:
                    prob = torch.exp(total_log_prob).item()
                    path_probs.append((prob, idx))
            if len(path_probs) > 0:
                probs_only = [p for p, _ in path_probs]
                path_probs_sorted = sorted(path_probs, key=lambda x: x[0], reverse=True)
                topk = path_probs_sorted[: min(5, len(path_probs_sorted))]
                bottomk = sorted(path_probs, key=lambda x: x[0])[: min(5, len(path_probs))]
                mean_prob = float(np.mean(probs_only)) if len(probs_only) > 0 else 0.0
                max_prob = float(topk[0][0]) if len(topk) > 0 else 0.0
                min_prob = float(bottomk[0][0]) if len(bottomk) > 0 else 0.0
                main_print(f"Path probability stats: mean={mean_prob:.4e}, max={max_prob:.4e}, min={min_prob:.4e}")
                if len(topk) > 0:
                    examples = []
                    for prob, i in topk:
                        r = leaf_nodes[i].reward
                        r_item = r.item() if isinstance(r, torch.Tensor) else float(r)
                        examples.append(f"prob={prob:.2e}, reward={r_item:.3f}")
                    main_print("Example high-probability paths: " + " | ".join(examples))
                if len(bottomk) > 0:
                    examples = []
                    for prob, i in bottomk:
                        r = leaf_nodes[i].reward
                        r_item = r.item() if isinstance(r, torch.Tensor) else float(r)
                        examples.append(f"prob={prob:.2e}, reward={r_item:.3f}")
                    main_print("Example low-probability paths: " + " | ".join(examples))
        except Exception as e:
            main_print(f"Failed to compute path probability statistics: {e}")

    # 2. Collect all nodes, group by depth
    all_nodes = []
    def collect_nodes(node):
        all_nodes.append(node)
        for child in node.children:
            collect_nodes(child)
    collect_nodes(root_node)
    
    # Group by depth
    max_depth = max(node.depth for node in all_nodes)
    nodes_by_depth = {depth: [] for depth in range(max_depth + 1)}
    for node in all_nodes:
        nodes_by_depth[node.depth].append(node)
    
    # 3. Compute intermediate node rewards from bottom up
    if use_prob_weighted:
        # é€å±‚å­è¾¹log_prob softmaxåŠ æƒèšåˆ
        alpha = 1.0  # æ¸©åº¦ç³»æ•°ï¼Œæœ€å°æ”¹åŠ¨ï¼šå…ˆå›ºå®šä¸º1.0
        for depth in reversed(range(max_depth)):  # ä»æœ€å¤§æ·±åº¦-1å¾€ä¸Š
            for node in nodes_by_depth[depth]:
                if not node.is_leaf():
                    children = node.children
                    if len(children) == 1:
                        # å•å­åˆ†æ”¯ï¼Œæƒé‡ä¸º1ï¼Œç›´æ¥ä¼ é€’
                        node.reward = children[0].reward
                    else:
                        # ä½¿ç”¨å­è¾¹çš„log_probåšsoftmaxæƒé‡
                        child_log_probs = []
                        child_rewards = []
                        for child in children:
                            # ä¿æŠ¤ï¼šè‹¥ç¼ºå°‘log_probï¼Œé€€åŒ–ä¸º0
                            lp = child.log_prob
                            lp_val = lp.squeeze().to(torch.float32) if lp is not None else torch.tensor(0.0, dtype=torch.float32)
                            child_log_probs.append(lp_val)
                            child_rewards.append(child.reward)
                        child_log_probs = torch.stack(child_log_probs)
                        # æ•°å€¼ç¨³å®šçš„softmax
                        weights = torch.softmax(alpha * child_log_probs, dim=0)
                        # å¯¹é½è®¾å¤‡ä¸dtype
                        device = weights.device
                        rewards_tensor = torch.stack([r.to(device=device, dtype=torch.float32) for r in child_rewards])
                        node.reward = torch.sum(weights * rewards_tensor)
    else:
        # åŸå§‹ï¼šå¯¹æ‰€æœ‰å¶å­åä»£çš„rewardåšç®€å•å¹³å‡
        for depth in reversed(range(max_depth)):  # ä»æœ€å¤§æ·±åº¦-1å¾€ä¸Š
            for node in nodes_by_depth[depth]:
                if not node.is_leaf():
                    leaf_descendants = node.get_all_leaf_descendants()
                    descendant_rewards = [leaf.reward for leaf in leaf_descendants]
                    node.reward = torch.stack(descendant_rewards).mean()
    
    main_print(f"Node reward computation finished: total {len(all_nodes)} nodes")
    return all_nodes, nodes_by_depth


def compute_hierarchical_advantages_by_depth(nodes_by_depth: Dict[int, List[TreeNode]]) -> Dict[str, torch.Tensor]:
    """
    æŒ‰æ·±åº¦åˆ†å±‚è®¡ç®—advantage
    åŒä¸€æ·±åº¦çš„èŠ‚ç‚¹ä¹‹é—´è¿›è¡Œç›¸å¯¹æ¯”è¾ƒ
    """
    node_advantages = {}
    
    for depth, nodes in nodes_by_depth.items():
        if len(nodes) <= 1:
            # å•ä¸ªèŠ‚ç‚¹çš„advantageè®¾ä¸º0
            for node in nodes:
                node.advantage = torch.tensor(0.0)  # ä¿®å¤ï¼šä½¿ç”¨æ ‡é‡tensorï¼Œåç»­ç»Ÿä¸€å¤„ç†å½¢çŠ¶
                node_advantages[node.node_id] = node.advantage
        else:
            # åŒå±‚å¤šä¸ªèŠ‚ç‚¹ï¼Œè®¡ç®—ç›¸å¯¹advantage
            rewards = torch.stack([node.reward for node in nodes])
            mean_reward = rewards.mean()
            std_reward = rewards.std()
            
            # ä¿®å¤ï¼šå¤„ç†æ‰€æœ‰rewardç›¸åŒçš„æƒ…å†µ
            if std_reward < 1e-6:  # å¦‚æœæ ‡å‡†å·®å¤ªå°ï¼Œè®¾ç½®æ‰€æœ‰advantageä¸º0
                advantages = torch.zeros_like(rewards)
                main_print(f"Warning: æ·±åº¦{depth}çš„æ‰€æœ‰rewardç›¸åŒ({mean_reward:.6f})ï¼Œè®¾ç½®advantageä¸º0")
            else:
                # æ ‡å‡†åŒ–å¾—åˆ°advantage
                advantages = (rewards - mean_reward) / (std_reward + 1e-8)
            
            for i, node in enumerate(nodes):
                node.advantage = advantages[i]  # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨æ ‡é‡ï¼Œä¿æŒä¸€è‡´çš„å½¢çŠ¶
                node_advantages[node.node_id] = node.advantage
    
    main_print(f"åˆ†å±‚advantageè®¡ç®—å®Œæˆï¼Œå…±{len(node_advantages)}ä¸ªèŠ‚ç‚¹")
    return node_advantages




def validate_tree_training_logic(leaf_nodes, advantages):
    """
    éªŒè¯æ ‘å½¢è®­ç»ƒé€»è¾‘çš„æ­£ç¡®æ€§
    """
    total_transitions = 0
    path_lengths = []
    
    for i, leaf_node in enumerate(leaf_nodes):
        path = leaf_node.get_path_from_root()
        path_length = len(path) - 1  # è½¬ç§»æ•°é‡
        path_lengths.append(path_length)
        total_transitions += path_length
        
        main_print(f"Leaf {i}: path_length={path_length}, advantage={advantages[i].item():.4f}")
    
    main_print(f"Total transitions to train: {total_transitions}")
    main_print(f"Average path length: {np.mean(path_lengths):.2f}")
    main_print(f"Path length range: [{min(path_lengths)}, {max(path_lengths)}]")
    
    return total_transitions


def grpo_one_step(
            args,
            latents,
            pre_latents,
            encoder_hidden_states, 
            pooled_prompt_embeds, 
            text_ids,
            image_ids,
            transformer,
            timesteps,
            i,
            sigma_schedule,
):
    B = encoder_hidden_states.shape[0]
    transformer.train()
    with torch.autocast("cuda", torch.bfloat16):
        pred= transformer(
            hidden_states=latents,
            encoder_hidden_states=encoder_hidden_states,
            timestep=timesteps/1000,
            guidance=torch.tensor(
                [3.5],
                device=latents.device,
                dtype=torch.bfloat16
            ),
            txt_ids=text_ids.repeat(encoder_hidden_states.shape[1],1),  # æ¢å¤åŸå§‹é€»è¾‘
            pooled_projections=pooled_prompt_embeds,
            img_ids=image_ids.squeeze(0) if image_ids.dim() == 3 else image_ids,
            joint_attention_kwargs=None,
            return_dict=False,
        )[0]
    # ä»…åœ¨åˆ†è£‚è¾¹ä¸Šè®­ç»ƒæ—¶ï¼Œlog_probè®¡ç®—ä»éœ€ä¸é‡‡æ ·åˆ†å¸ƒä¸€è‡´ã€‚
    # ä¸ºä¿æŒç®€å•ï¼Œè¿™é‡Œä»å¤ç”¨SDEå¼ log_probï¼›å½“è°ƒç”¨æ–¹åªä¼ å…¥åˆ†è£‚è¾¹æ—¶ä¸ä¼šå¼•å…¥ä¸ä¸€è‡´ã€‚
    z, pred_original, log_prob = flux_step_with_split(
        pred, latents.to(torch.float32), args.eta, sigma_schedule, i, 
        prev_sample=pre_latents.to(torch.float32), grpo=True, sde_solver=True,
        split_noise_scale=args.tree_split_noise_scale
    )
    return log_prob


def sample_reference_model_tree(
    args,
    device, 
    transformer,
    vae,
    encoder_hidden_states, 
    pooled_prompt_embeds, 
    text_ids,
    reward_model,
    tokenizer,
    caption,
    preprocess_val,
):
    """
    ä½¿ç”¨åˆ†è£‚å¼æ ‘å½¢ç»“æ„è¿›è¡Œå‚è€ƒæ¨¡å‹é‡‡æ ·
    """
    w, h, t = args.w, args.h, args.t
    sample_steps = args.sampling_steps    # 20
    sigma_schedule = torch.linspace(1, 0, args.sampling_steps + 1)
    
    sigma_schedule = sd3_time_shift(args.shift, sigma_schedule)

    assert_eq(
        len(sigma_schedule),
        sample_steps + 1,
        "sigma_schedule must have length sample_steps + 1",
    )

    B = encoder_hidden_states.shape[0]
    SPATIAL_DOWNSAMPLE = 8
    IN_CHANNELS = 16
    latent_w, latent_h = w // SPATIAL_DOWNSAMPLE, h // SPATIAL_DOWNSAMPLE

    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å®é™…çš„batch_sizeè€Œä¸æ˜¯ç¡¬ç¼–ç çš„1
    # æ–°é€»è¾‘ï¼šè¾“å…¥æ˜¯åŸå§‹çš„promptæ•°é‡ï¼Œæ¯ä¸ªpromptç”Ÿæˆä¸€ä¸ªæ ‘
    if args.use_group:
        # use_groupæ¨¡å¼ï¼šæ¯ä¸ªåŸå§‹promptç”Ÿæˆä¸€ä¸ªæ ‘ï¼Œæ ‘æœ‰num_generationsä¸ªå¶å­èŠ‚ç‚¹
        batch_size = 1  # æ¯æ¬¡å¤„ç†ä¸€ä¸ªåŸå§‹prompt
    else:
        # éuse_groupæ¨¡å¼ï¼šæ¯ä¸ªè¾“å…¥æ ·æœ¬ç”Ÿæˆä¸€ä¸ªæ ‘
        batch_size = min(B, args.train_batch_size)
    
    batch_indices = torch.chunk(torch.arange(B), max(1, B // batch_size))

    all_rewards = []  
    all_leaf_nodes = []
    
    # å¯¹äºæ ‘å½¢é‡‡æ ·ï¼Œæˆ‘ä»¬æœŸæœ›æœ€ç»ˆå¾—åˆ°16ä¸ªåˆ†æ”¯
    target_final_branches = args.num_generations if hasattr(args, 'num_generations') else 16
    num_split_rounds = int(math.log2(target_final_branches))  # 4è½®åˆ†è£‚å¾—åˆ°16ä¸ªåˆ†æ”¯
    
    if args.init_same_noise:
        input_latents = torch.randn(
                (1, IN_CHANNELS, latent_h, latent_w),
                device=device,
                dtype=torch.bfloat16,
            )

    for index, batch_idx in enumerate(batch_indices):
        batch_encoder_hidden_states = encoder_hidden_states[batch_idx]
        batch_pooled_prompt_embeds = pooled_prompt_embeds[batch_idx]
        batch_text_ids = text_ids[batch_idx]
        batch_caption = [caption[i] for i in batch_idx]
        
        if not args.init_same_noise:
            input_latents = torch.randn(
                    (len(batch_idx), IN_CHANNELS, latent_h, latent_w),
                    device=device,
                    dtype=torch.bfloat16,
                )
        else:
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä½¿ç”¨ç›¸åŒå™ªå£°ä½†batch_size > 1ï¼Œéœ€è¦é‡å¤latents
            if len(batch_idx) > 1:
                input_latents = input_latents.repeat(len(batch_idx), 1, 1, 1)
        
        input_latents_new = pack_latents(input_latents, len(batch_idx), IN_CHANNELS, latent_h, latent_w)
        image_ids = prepare_latent_image_ids(len(batch_idx), latent_h // 2, latent_w // 2, device, torch.bfloat16)
        
        progress_bar = tqdm(range(0, sample_steps), desc="Tree Sampling Progress")
        
        with torch.no_grad():
            final_latents, pred_original, path_log_probs, leaf_nodes, all_log_probs_tree = run_tree_sample_step(
                args,
                input_latents_new,
                progress_bar,
                sigma_schedule,
                transformer,
                batch_encoder_hidden_states,
                batch_pooled_prompt_embeds,
                batch_text_ids,
                image_ids,
                grpo_sample=True,
                num_final_branches=target_final_branches,
                num_split_rounds=num_split_rounds,
                batch_offset=batch_idx[0] if len(batch_idx) > 0 else 0, # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨batch_idxçš„ç¬¬ä¸€ä¸ªå…ƒç´ 
            )
        
        all_leaf_nodes.extend(leaf_nodes)
        vae.enable_tiling()
        
        image_processor = VaeImageProcessor(16)
        rank = int(os.environ["RANK"])

        # å¤„ç†æ¯ä¸ªå¶å­èŠ‚ç‚¹ç”Ÿæˆçš„å›¾åƒ
        batch_rewards = []
        for leaf_idx, leaf_node in enumerate(leaf_nodes):
            latent = pred_original[leaf_idx:leaf_idx+1]
            with torch.inference_mode():
                with torch.autocast("cuda", dtype=torch.bfloat16):
                    unpacked_latent = unpack_latents(latent, h, w, 8)
                    unpacked_latent = (unpacked_latent / 0.3611) + 0.1159
                    image = vae.decode(unpacked_latent, return_dict=False)[0]
                    decoded_image = image_processor.postprocess(image)
            
            # ä¿å­˜å›¾åƒåˆ°å®éªŒç‰¹å®šç›®å½•
            exp_name = getattr(args, '_exp_name', 'default')
            image_dir = f"images_branchgrpo/{exp_name}/rank_{rank}"
            os.makedirs(image_dir, exist_ok=True)
            image_path = f"{image_dir}/flux_branchgrpo_{index}_{leaf_idx}.png"
            decoded_image[0].save(image_path)

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ¹æ®leaf_nodeçš„batch_idxè·å–æ­£ç¡®çš„caption
            leaf_batch_idx = leaf_node.batch_idx
            # ä»åŸå§‹captionåˆ—è¡¨ä¸­è·å–å¯¹åº”çš„caption
            correct_caption = caption[leaf_batch_idx] if leaf_batch_idx < len(caption) else batch_caption[0]
            
            # ğŸ”§ æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆåªæ‰“å°å‰å‡ ä¸ªæ ·æœ¬ï¼‰
            if leaf_idx < 5 or leaf_idx % 16 == 0:  # æ¯ç»„çš„ç¬¬ä¸€ä¸ªæ ·æœ¬
                main_print(f"Leaf {leaf_idx}: batch_idx={leaf_batch_idx}, caption='{correct_caption[:50]}...'")

            # è®¡ç®—å¥–åŠ±
            if args.use_hpsv2:
                with torch.no_grad():
                    image_pil = decoded_image[0]
                    image_tensor = preprocess_val(image_pil).unsqueeze(0).to(device=device, non_blocking=True)
                    # ğŸ”§ ä½¿ç”¨æ­£ç¡®çš„captionè€Œä¸æ˜¯æ€»æ˜¯batch_caption[0]
                    text = tokenizer([correct_caption]).to(device=device, non_blocking=True)
                    with torch.amp.autocast('cuda'):
                        outputs = reward_model(image_tensor, text)
                        image_features, text_features = outputs["image_features"], outputs["text_features"]
                        logits_per_image = image_features @ text_features.T
                        hps_score = torch.diagonal(logits_per_image)
                    batch_rewards.append(hps_score)
            
            if args.use_pickscore:
                def calc_probs(processor, model, prompt, images, device):
                    image_inputs = processor(
                        images=images,
                        padding=True,
                        truncation=True,
                        max_length=77,
                        return_tensors="pt",
                    ).to(device)
                    text_inputs = processor(
                        text=prompt,
                        padding=True,
                        truncation=True,
                        max_length=77,
                        return_tensors="pt",
                    ).to(device)
                    with torch.no_grad():
                        image_embs = model.get_image_features(**image_inputs)
                        image_embs = image_embs / torch.norm(image_embs, dim=-1, keepdim=True)
                    
                        text_embs = model.get_text_features(**text_inputs)
                        text_embs = text_embs / torch.norm(text_embs, dim=-1, keepdim=True)
                    
                        scores = (text_embs @ image_embs.T)[0]
                    
                    return scores
                
                pil_images = [Image.open(image_path)]
                # ğŸ”§ ä½¿ç”¨æ­£ç¡®çš„caption
                score = calc_probs(tokenizer, reward_model, correct_caption, pil_images, device)
                batch_rewards.append(score)

        all_rewards.extend(batch_rewards)

    all_rewards = torch.stack(all_rewards) if all_rewards else torch.tensor([])
    
    return all_rewards, path_log_probs, all_leaf_nodes, sigma_schedule, all_log_probs_tree


def gather_tensor(tensor):
    if not dist.is_initialized():
        return tensor
    world_size = dist.get_world_size()
    gathered_tensors = [torch.zeros_like(tensor) for _ in range(world_size)]
    dist.all_gather(gathered_tensors, tensor)
    return torch.cat(gathered_tensors, dim=0)


def train_one_step_tree(
    args,
    device,
    transformer,
    vae,
    reward_model,
    tokenizer,
    optimizer,
    lr_scheduler,
    loader,
    noise_scheduler,
    max_grad_norm,
    preprocess_val,
    current_step=None,
    max_steps=None,
):
    """
    ä½¿ç”¨åˆ†è£‚å¼æ ‘å½¢ç»“æ„çš„è®­ç»ƒæ­¥éª¤
    """
    total_loss = 0.0
    
    # ğŸ•’ æ—¶é—´è®°å½•ï¼šå¼€å§‹è®¡æ—¶
    step_start_time = time.time()
    
    (
        encoder_hidden_states, 
        pooled_prompt_embeds, 
        text_ids,
        caption,
    ) = next(loader)

    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¸è¦åœ¨é‡‡æ ·å‰è¿›è¡Œrepeatï¼Œä¿æŒåŸå§‹çš„promptæ•°é‡
    # åŸå§‹çš„encoder_hidden_states: [2, 512, 4096] (2ä¸ªä¸åŒçš„prompt)
    original_batch_size = encoder_hidden_states.shape[0]
    
    # ğŸ•’ æ—¶é—´è®°å½•ï¼šé‡‡æ ·å¼€å§‹
    sample_start_time = time.time()
    
    # ä½¿ç”¨æ ‘å½¢é‡‡æ ·ç”Ÿæˆæ ·æœ¬ï¼ˆæ¯ä¸ªåŸå§‹promptç”Ÿæˆä¸€ä¸ªæ ‘ï¼‰
    rewards, path_log_probs, leaf_nodes, sigma_schedule, all_log_probs_tree = sample_reference_model_tree(
        args,
        device, 
        transformer,
        vae,
        encoder_hidden_states,  # ä¿æŒåŸå§‹ç»´åº¦ [2, 512, 4096]
        pooled_prompt_embeds,   # ä¿æŒåŸå§‹ç»´åº¦ [2, ...]
        text_ids,               # ä¿æŒåŸå§‹ç»´åº¦ [2, 3]
        reward_model,
        tokenizer,
        caption,
        preprocess_val,
    )
    
    # ğŸ•’ æ—¶é—´è®°å½•ï¼šé‡‡æ ·ç»“æŸï¼Œå¥–åŠ±è®¡ç®—å¼€å§‹
    sample_end_time = time.time()
    reward_start_time = sample_end_time
    
    # ğŸ”§ æ ‘é‡‡æ ·å®Œæˆåï¼Œåº”ç”¨use_groupé€»è¾‘
    if args.use_group:
        # éªŒè¯é‡‡æ ·ç»“æœçš„æ•°é‡
        expected_samples = original_batch_size * args.num_generations
        actual_samples = len(rewards)
        main_print(f"Use_group mode: expected {expected_samples} samples, got {actual_samples}")
        
        if actual_samples != expected_samples:
            main_print(f"Warning: Sample count mismatch! Expected {expected_samples}, got {actual_samples}")
        
        # ğŸ”§ éªŒè¯rewardå’Œleaf_nodesçš„batch_idxå¯¹åº”å…³ç³»
        main_print("Verifying reward-batch_idx correspondence:")
        for i in range(min(10, len(leaf_nodes))):  # åªæ‰“å°å‰10ä¸ªæ ·æœ¬
            leaf = leaf_nodes[i]
            reward = rewards[i].item() if hasattr(rewards[i], 'item') else rewards[i]
            main_print(f"  Sample {i}: batch_idx={leaf.batch_idx}, reward={reward:.4f}")
        
        # ä¸ºcaptionåº”ç”¨use_groupé€»è¾‘ï¼ˆç”¨äºrewardè®¡ç®—æ—¶çš„åŒ¹é…ï¼‰
        if isinstance(caption, str):
            caption = [caption] * args.num_generations
        elif isinstance(caption, list):
            caption = [item for item in caption for _ in range(args.num_generations)]
        else:
            raise ValueError(f"Unsupported caption type: {type(caption)}")
    
    # æ”¶é›†åˆ†å¸ƒå¼å¥–åŠ±
    gathered_reward = gather_tensor(rewards)
    if dist.get_rank() == 0:
        print("gathered_reward (tree)", gathered_reward)
        exp_name = getattr(args, '_exp_name', 'default')
        log_dir = f"log/{exp_name}"
        os.makedirs(log_dir, exist_ok=True)
        with open(f'{log_dir}/reward_tree.txt', 'a') as f: 
            f.write(f"{gathered_reward.mean().item()}\n")
    
    # ğŸ•’ æ—¶é—´è®°å½•ï¼šå¥–åŠ±è®¡ç®—ç»“æŸ
    reward_end_time = time.time()

    # ğŸŒŸ æ·»åŠ  use_group çš„ advantage è®¡ç®—é€»è¾‘
    # 1. ä»å¶å­èŠ‚ç‚¹æ”¶é›†æ‰€æœ‰æ ¹èŠ‚ç‚¹
    root_nodes = []
    processed_roots = set()
    for leaf in leaf_nodes:
        current = leaf
        while current.parent is not None:
            current = current.parent
        if current.node_id not in processed_roots:
            root_nodes.append(current)
            processed_roots.add(current.node_id)
    
    main_print(f"Found {len(root_nodes)} root nodes (for {len(leaf_nodes)} leaf nodes)")
    
    # ğŸ”§ æ·»åŠ è°ƒè¯•ä¿¡æ¯éªŒè¯batch_idxåˆ†é…
    if len(leaf_nodes) > 0:
        batch_idx_counts = {}
        for leaf in leaf_nodes:
            batch_idx = leaf.batch_idx
            batch_idx_counts[batch_idx] = batch_idx_counts.get(batch_idx, 0) + 1
        main_print(f"Leaf nodes by batch_idx: {batch_idx_counts}")
        
        if args.use_group:
            expected_per_batch = args.num_generations
            for batch_idx, count in batch_idx_counts.items():
                if count != expected_per_batch:
                    main_print(f"Warning: batch_idx {batch_idx} has {count} leaf nodes, expected {expected_per_batch}")
    
    if args.use_group:
        # ğŸ”§ ä¿®å¤ï¼šæŒ‰batch_idxå¯¹leaf_nodesæ’åºï¼Œç¡®ä¿æ­£ç¡®çš„åˆ†ç»„
        # é¦–å…ˆæŒ‰batch_idxæ’åºå¶å­èŠ‚ç‚¹
        leaf_nodes_sorted = sorted(leaf_nodes, key=lambda node: node.batch_idx)
        rewards_sorted = []
        
        # é‡æ–°æ’åºrewardsä»¥åŒ¹é…æ’åºåçš„å¶å­èŠ‚ç‚¹
        batch_leaf_mapping = {}  # batch_idx -> list of (original_idx, leaf_node)
        for i, leaf in enumerate(leaf_nodes):
            batch_idx = leaf.batch_idx
            if batch_idx not in batch_leaf_mapping:
                batch_leaf_mapping[batch_idx] = []
            batch_leaf_mapping[batch_idx].append((i, leaf))
        
        # æŒ‰batch_idxé¡ºåºé‡æ„rewards
        for batch_idx in sorted(batch_leaf_mapping.keys()):
            batch_leaves = batch_leaf_mapping[batch_idx]
            for original_idx, leaf in batch_leaves:
                rewards_sorted.append(rewards[original_idx])
        rewards_sorted = torch.stack(rewards_sorted)
        
        # éªŒè¯æ’åºæ­£ç¡®æ€§
        main_print("Verifying reward-batch_idx correspondence after sorting:")
        for i in range(min(10, len(leaf_nodes_sorted))):  # åªæ‰“å°å‰10ä¸ªæ ·æœ¬
            leaf = leaf_nodes_sorted[i]
            reward = rewards_sorted[i].item() if hasattr(rewards_sorted[i], 'item') else rewards_sorted[i]
            main_print(f"  Sample {i}: batch_idx={leaf.batch_idx}, reward={reward:.4f}")
        
        # æŒ‰ç»„è®¡ç®—advantageï¼Œç±»ä¼¼åŸå§‹GRPO
        n = len(rewards_sorted) // args.num_generations
        leaf_advantages = torch.zeros_like(rewards_sorted)
        
        for i in range(n):
            start_idx = i * args.num_generations
            end_idx = (i + 1) * args.num_generations
            group_rewards = rewards_sorted[start_idx:end_idx]
            group_mean = group_rewards.mean()
            group_std = group_rewards.std() + 1e-8
            leaf_advantages[start_idx:end_idx] = (group_rewards - group_mean) / group_std
        
        # ä¸ºå¶å­èŠ‚ç‚¹åˆ†é…groupè®¡ç®—çš„advantage
        for i, leaf_node in enumerate(leaf_nodes_sorted):
            leaf_node.advantage = leaf_advantages[i]
            
        # use_groupæ¨¡å¼ä¸‹ï¼Œæ„å»ºèŠ‚ç‚¹å±‚æ¬¡ï¼ˆå¤„ç†æ‰€æœ‰æ ¹èŠ‚ç‚¹ï¼‰
        all_nodes = []
        def collect_nodes(node):
            all_nodes.append(node)
            for child in node.children:
                collect_nodes(child)
        
        for root_node in root_nodes:
            collect_nodes(root_node)
        
        # æ„å»ºnode_advantageså­—å…¸
        node_advantages = {}
        for node in all_nodes:
            if hasattr(node, 'advantage'):
                node_advantages[node.node_id] = node.advantage
            else:
                node_advantages[node.node_id] = torch.tensor(0.0)
                
        main_print(f"ä½¿ç”¨use_groupæ¨¡å¼è®¡ç®—advantage: {n}ç»„ï¼Œæ¯ç»„{args.num_generations}ä¸ªç”Ÿæˆ")
    else:
        # åŸå§‹çš„åˆ†å±‚advantageè®¡ç®—ï¼ˆæ¯æ£µæ ‘å•ç‹¬å¤„ç†ï¼Œé¿å…è·¨æ ·æœ¬ä¿¡æ¯æ³„éœ²ï¼‰
        all_nodes = []
        node_advantages = {}

        # ğŸ”§ ä¿®å¤ï¼šæŒ‰batch_idxç»„ç»‡leaf_nodeså’Œrewardsçš„å¯¹åº”å…³ç³»
        batch_leaf_mapping = {}  # batch_idx -> list of (leaf_node, reward)
        for i, leaf in enumerate(leaf_nodes):
            batch_idx = leaf.batch_idx
            if batch_idx not in batch_leaf_mapping:
                batch_leaf_mapping[batch_idx] = []
            batch_leaf_mapping[batch_idx].append((leaf, rewards[i]))

        max_depth_across_trees = 0

        # ä¸ºæ¯ä¸ªæ ¹èŠ‚ç‚¹æ­£ç¡®åˆ†é…å¯¹åº”çš„å¶å­èŠ‚ç‚¹rewardsï¼Œå¹¶åˆ†åˆ«è®¡ç®—advantage
        for root_node in root_nodes:
            # è·å–å½“å‰æ ¹èŠ‚ç‚¹çš„æ‰€æœ‰å¶å­åä»£
            root_leaf_descendants = root_node.get_all_leaf_descendants()
            root_batch_idx = root_node.batch_idx

            # ğŸ”§ ä»batch_leaf_mappingè·å–å¯¹åº”çš„rewards
            if root_batch_idx in batch_leaf_mapping:
                batch_leaf_rewards = batch_leaf_mapping[root_batch_idx]
                # ç¡®ä¿å¶å­èŠ‚ç‚¹é¡ºåºåŒ¹é…
                root_rewards = []
                for leaf_desc in root_leaf_descendants:
                    for leaf_node, reward in batch_leaf_rewards:
                        if leaf_desc.node_id == leaf_node.node_id:
                            root_rewards.append(reward)
                            break
                root_rewards = torch.stack(root_rewards) if len(root_rewards) > 0 else torch.zeros(len(root_leaf_descendants), device=rewards.device)
            else:
                main_print(f"Warning: No rewards found for batch_idx {root_batch_idx}")
                root_rewards = torch.zeros(len(root_leaf_descendants), device=rewards.device)

            # 2. è®¡ç®—å½“å‰æ ‘çš„æ‰€æœ‰èŠ‚ç‚¹çš„rewardï¼ˆè‡ªåº•å‘ä¸Šèšåˆï¼‰
            tree_nodes, tree_nodes_by_depth = compute_node_rewards_from_leaves(
                root_node,
                root_rewards,
                root_leaf_descendants,
                use_prob_weighted=getattr(args, "tree_prob_weighted", False),
            )
            all_nodes.extend(tree_nodes)

            # 3. æŒ‰æ·±åº¦åˆ†å±‚è®¡ç®—advantageï¼ˆä»…åœ¨è¯¥æ ‘å†…éƒ¨å½’ä¸€åŒ–ï¼‰
            tree_node_advantages = compute_hierarchical_advantages_by_depth(tree_nodes_by_depth)
            node_advantages.update(tree_node_advantages)

            # è®°å½•è¯¥æ ‘çš„æœ€å¤§æ·±åº¦
            if len(tree_nodes_by_depth) > 0:
                max_depth_across_trees = max(max_depth_across_trees, max(tree_nodes_by_depth.keys()))

        main_print(f"Tree depth (max across trees): {max_depth_across_trees}")
    
    main_print(f"Tree training: {len(leaf_nodes)} leaf nodes, {len(all_nodes)} total nodes")
    main_print(f"Leaf rewards range: [{rewards.min():.4f}, {rewards.max():.4f}]")
    

    
    # ğŸŒŸ ç»Ÿä¸€çš„æ ‘å½¢è®­ç»ƒé€»è¾‘ - ä¿®æ”¹ä¸ºä¸åŸå§‹GRPOä¸€è‡´çš„æ¢¯åº¦ç´¯ç§¯
    # ç°åœ¨æ‰€æœ‰è½¬ç§»éƒ½æ˜¯æ ‘ä¸­çš„çˆ¶å­å…³ç³»ï¼ŒåŒ…æ‹¬åˆ†è£‚å’Œéåˆ†è£‚æ­¥éª¤
    
    training_samples = []
    
    # æ”¶é›†æ‰€æœ‰æ ‘è½¬ç§»ï¼ˆç°åœ¨åŒ…æ‹¬åˆ†è£‚è½¬ç§»å’Œè¿ç»­è½¬ç§»ï¼‰
    def collect_all_transitions(node):
        for child in node.children:
            # åˆ¤æ–­è½¬ç§»ç±»å‹ï¼šå¤šä¸ªå­èŠ‚ç‚¹=åˆ†è£‚ï¼Œå•ä¸ªå­èŠ‚ç‚¹=è¿ç»­
            transition_type = "split" if len(node.children) > 1 else "sequential"
            
            child_advantage = node_advantages[child.node_id]
            sample = {
                "latent": node.latent,                     # çˆ¶èŠ‚ç‚¹çŠ¶æ€
                "next_latent": child.latent,               # å­èŠ‚ç‚¹çŠ¶æ€  
                "log_prob": child.log_prob,                # è½¬ç§»çš„log_prob
                "advantage": child_advantage,              # å­èŠ‚ç‚¹çš„advantage
                "step": node.step,                         # çˆ¶èŠ‚ç‚¹çš„æ—¶é—´æ­¥
                "batch_idx": node.batch_idx,               # ğŸ”§ æ·»åŠ batch_idxä¿¡æ¯
                "parent_id": node.node_id,                 # ç”¨äºè°ƒè¯•
                "child_id": child.node_id,                 # ç”¨äºè°ƒè¯•
                "child_depth": child.depth,                # ç”¨äºè°ƒè¯•
                "transition_type": transition_type,        # è½¬ç§»ç±»å‹
                "is_sde_edge": getattr(child, "is_sde_edge", transition_type=="split"),
            }
            training_samples.append(sample)
            
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            collect_all_transitions(child)
    
    # ğŸ”§ ä¿®å¤ï¼šå¯¹æ‰€æœ‰æ ¹èŠ‚ç‚¹è°ƒç”¨collect_all_transitions
    for root_node in root_nodes:
        collect_all_transitions(root_node)
    
    # éªŒè¯æ ‘ç»“æ„çš„å®Œæ•´æ€§
    total_nodes = len(all_nodes)
    leaf_count = len([node for node in all_nodes if node.is_leaf()])
    main_print(f"æ ‘ç»“æ„éªŒè¯: æ€»èŠ‚ç‚¹æ•°={total_nodes}, å¶å­èŠ‚ç‚¹æ•°={leaf_count}")

    # ğŸŒ— æ··åˆæ¨¡å¼ï¼šä»…è®­ç»ƒSDEè¾¹ï¼ˆåˆ†è£‚è¾¹å§‹ç»ˆSDEï¼›è‹¥å¯ç”¨æ»‘åŠ¨çª—å£ï¼Œçª—å£å†…éåˆ†è£‚è¾¹ä¹Ÿä½œä¸ºSDEï¼‰
    if getattr(args, 'mix_ode_sde_tree', False):
        before = len(training_samples)
        # åŸºäºçª—å£ä¸åˆ†è£‚ç‚¹ï¼ŒæŒ‰ step å†æ¬¡ç¡®å®šåº”ä¿ç•™çš„ SDE æ­¥
        total_steps = int(args.sampling_steps)
        stride = max(1, int(getattr(args, 'depth_pruning_slide_interval', 1)))
        window_size = max(1, int(getattr(args, 'mix_sde_window_size', 4)))
        sde_steps = set()
        # çª—å£å—ï¼šä» 0, stride, 2*stride, ... å¼€å§‹ï¼Œå„è‡ªè¦†ç›– window_size æ­¥
        start_step = 0
        while start_step < total_steps:
            end_step = min(total_steps - 1, start_step + window_size - 1)
            for s in range(start_step, end_step + 1):
                sde_steps.add(s)
            start_step += stride
        # åˆ†è£‚ç‚¹åŠ å…¥ SDE
        split_points = parse_split_points(args, total_steps)
        for sp in split_points:
            sde_steps.add(sp)
        # è¿‡æ»¤ï¼šä»…ä¿ç•™çˆ¶æ­¥åœ¨ sde_steps çš„è¾¹
        training_samples = [s for s in training_samples if (s.get("step", -1) in sde_steps)]
        # ç»Ÿè®¡åˆ†å¸ƒ
        split_cnt = sum(1 for s in training_samples if s["transition_type"] == "split")
        seq_cnt = len(training_samples) - split_cnt
        main_print(f"æ··åˆæ¨¡å¼å¯ç”¨ï¼šä»…ä½¿ç”¨SDEè¾¹è®­ç»ƒ {before} -> {len(training_samples)} (split={split_cnt}, sequential={seq_cnt}), sde_steps={sorted(list(sde_steps))}")
    
    # ğŸ• æ­¥éª¤è£å‰ªæ§åˆ¶ï¼šåˆ¤æ–­å½“å‰æ­¥éª¤æ˜¯å¦åº”è¯¥è¿›è¡Œè£å‰ª
    should_prune = True
    if current_step is not None and max_steps is not None and hasattr(args, 'pruning_step_ratio'):
        pruning_step_ratio = args.pruning_step_ratio
        pruning_cutoff_step = int(max_steps * pruning_step_ratio)
        should_prune = current_step <= pruning_cutoff_step
        main_print(f"ğŸ• è£å‰ªæ­¥éª¤æ§åˆ¶: å½“å‰æ­¥éª¤ {current_step}/{max_steps}, è£å‰ªæˆªæ­¢æ­¥éª¤ {pruning_cutoff_step}, æ˜¯å¦è£å‰ª: {should_prune}")
    else:
        main_print(f"ğŸ• è£å‰ªæ­¥éª¤æ§åˆ¶: æœªè®¾ç½®æ­¥éª¤ä¿¡æ¯ï¼Œé»˜è®¤è¿›è¡Œè£å‰ª")
    
    # ğŸŒ¿ æ·±åº¦è£å‰ªé€»è¾‘ï¼šæ”¯æŒæ»‘åŠ¨çª—å£ä¸å›ºå®šçª—å£
    original_sample_count = len(training_samples)
    if should_prune and hasattr(args, 'depth_pruning') and args.depth_pruning:
        try:
            base_depths = [int(d.strip()) for d in args.depth_pruning.split(',') if d.strip()]
            base_depths = sorted(base_depths)

            active_pruning_depths = base_depths

            # å¯é€‰ï¼šæ»‘åŠ¨çª—å£
            if getattr(args, 'depth_pruning_slide', False):
                # è‡ªåŠ¨æ¨æ–­â€œåœæ­¢æ»‘åŠ¨æ·±åº¦â€ï¼šæœ€åä¸€æ¬¡åˆ†è£‚çˆ¶èŠ‚ç‚¹æ·±åº¦ = max(split_points)
                sampling_steps = args.sampling_steps
                split_points = parse_split_points(args, sampling_steps)
                auto_stop_depth = max(split_points) if len(split_points) > 0 else 0
                stop_depth = auto_stop_depth
                if hasattr(args, 'depth_pruning_stop_depth') and args.depth_pruning_stop_depth is not None:
                    stop_depth = args.depth_pruning_stop_depth

                interval = max(1, int(getattr(args, 'depth_pruning_slide_interval', 1)))
                # åœ¨ t ä¸ªè®­ç»ƒ step åæ»‘åŠ¨ä¸€æ¬¡ï¼šshift = current_step // interval
                shift_now = max(0, int(current_step // interval))
                # é™åˆ¶æœ€å¤§æ»‘åŠ¨æ¬¡æ•°ï¼šçª—å£æœ€æµ…å±‚ä¸èƒ½é«˜äº stop_depth
                max_shift = max(0, base_depths[0] - stop_depth)
                shift_now = min(shift_now, max_shift)

                active_pruning_depths = [d - shift_now for d in base_depths]
                main_print(
                    f"ğŸŒ¿ æ·±åº¦è£å‰ª(æ»‘åŠ¨çª—å£): step={current_step}, interval={interval}, shift={shift_now}, "
                    f"window {base_depths} -> {active_pruning_depths}, stop_at_depth={stop_depth}"
                )

            pruning_depths = set(active_pruning_depths)

            if pruning_depths:
                # è¿‡æ»¤æ‰æŒ‡å®šæ·±åº¦çš„è®­ç»ƒæ ·æœ¬
                filtered_samples = []
                pruned_count = 0
                for sample in training_samples:
                    child_depth = sample["child_depth"]
                    if child_depth not in pruning_depths:
                        filtered_samples.append(sample)
                    else:
                        pruned_count += 1

                training_samples = filtered_samples
                main_print(f"ğŸŒ¿ æ·±åº¦è£å‰ª: è£å‰ªæ·±åº¦ {sorted(pruning_depths)}")
                main_print(f"   è£å‰ªå‰æ ·æœ¬æ•°: {original_sample_count}")
                main_print(f"   è£å‰ªåæ ·æœ¬æ•°: {len(training_samples)}")
                main_print(f"   è£å‰ªæ ·æœ¬æ•°: {pruned_count}")
                main_print(f"   è£å‰ªæ¯”ä¾‹: {pruned_count/original_sample_count*100:.1f}%")
        except ValueError as e:
            main_print(f"Warning: æ·±åº¦è£å‰ªå‚æ•°è§£æå¤±è´¥: {e}")
    else:
        main_print(f"è®­ç»ƒæ ·æœ¬æ€»æ•°: {original_sample_count} (æ— æ·±åº¦è£å‰ª)")
    
    # ğŸŒ³ å®½åº¦è£å‰ªé€»è¾‘ï¼šåœ¨æ·±åº¦è£å‰ªåè¿›è¡Œï¼Œä¿ç•™æŒ‡å®šæ¯”ä¾‹çš„è®­ç»ƒæ ·æœ¬
    samples_after_depth_pruning = len(training_samples)
    if should_prune and hasattr(args, 'width_pruning_mode') and args.width_pruning_mode is not None and args.width_pruning_mode > 0:
        try:
            width_pruning_ratio = getattr(args, 'width_pruning_ratio', 0.5)  # é»˜è®¤ä¿ç•™50%
            mode = args.width_pruning_mode
            
            # ğŸ” é¦–å…ˆæ‰¾åˆ°æœ€åä¸€æ¬¡åˆ†è£‚çš„æ­¥éª¤
            # é‡æ–°è®¡ç®—total_stepsï¼ˆä¸é‡‡æ ·æ—¶ä¸€è‡´ï¼‰
            sampling_steps = args.sampling_steps
            split_points = parse_split_points(args, sampling_steps)
            if not split_points:
                main_print("Warning: æ²¡æœ‰åˆ†è£‚ç‚¹ï¼Œè·³è¿‡å®½åº¦è£å‰ª")
            else:
                last_split_step = max(split_points)
                main_print(f"ğŸ” æœ€åä¸€æ¬¡åˆ†è£‚æ­¥éª¤: {last_split_step}")
                
                # ğŸ¯ è¯†åˆ«æœ€åä¸€å±‚çˆ¶èŠ‚ç‚¹çš„åç»­è½¬ç§»ï¼ˆstep > last_split_stepçš„è½¬ç§»ï¼‰
                last_layer_samples = []
                other_samples = []
                
                for sample in training_samples:
                    if sample["step"] > last_split_step:
                        last_layer_samples.append(sample)
                    else:
                        other_samples.append(sample)
                
                main_print(f"ğŸ¯ æœ€åä¸€å±‚è½¬ç§»æ ·æœ¬æ•°: {len(last_layer_samples)}")
                main_print(f"ğŸ¯ å…¶ä»–å±‚è½¬ç§»æ ·æœ¬æ•°: {len(other_samples)}")
                
                if len(last_layer_samples) == 0:
                    main_print("Warning: æ²¡æœ‰æœ€åä¸€å±‚è½¬ç§»æ ·æœ¬ï¼Œè·³è¿‡å®½åº¦è£å‰ª")
                    total_pruned = 0
                else:
                    if mode == 1:
                        # æ–¹å¼1ï¼šæŒ‰æœ€ååˆ†è£‚äº§ç”Ÿçš„åˆ†æ”¯åˆ†ç»„ï¼Œä¿ç•™æ¯ä¸ªåˆ†æ”¯æœ€å¥½çš„åç»­è½¬ç§»
                        main_print(f"ğŸŒ³ å®½åº¦è£å‰ªæ¨¡å¼1: ä¿ç•™æ¯ä¸ªæœ€ååˆ†è£‚åˆ†æ”¯æœ€å¥½çš„{width_pruning_ratio*100:.0f}%åç»­è½¬ç§»")
                        
                        # ğŸ” æŒ‰æœ€ååˆ†è£‚äº§ç”Ÿçš„åˆ†æ”¯åˆ†ç»„
                        # sample["step"]æ˜¯çˆ¶èŠ‚ç‚¹çš„æ­¥éª¤ï¼Œsample["parent_id"]æ˜¯çˆ¶èŠ‚ç‚¹çš„ID
                        
                        def find_last_split_branch(sample, all_nodes_dict):
                            """è¿½æº¯æ ·æœ¬åˆ°æœ€ååˆ†è£‚æ­¥éª¤çš„åˆ†æ”¯"""
                            current_node_id = sample["parent_id"]
                            target_step = last_split_step + 1  # æœ€ååˆ†è£‚äº§ç”Ÿçš„å­èŠ‚ç‚¹çš„æ­¥éª¤
                            
                            # å¦‚æœçˆ¶èŠ‚ç‚¹å°±æ˜¯æœ€ååˆ†è£‚äº§ç”Ÿçš„èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
                            if sample["step"] == target_step:
                                return current_node_id
                            
                            # å¦åˆ™å‘ä¸Šè¿½æº¯
                            while current_node_id in all_nodes_dict:
                                current_node = all_nodes_dict[current_node_id]
                                if current_node.step == target_step:
                                    return current_node_id
                                if current_node.parent is not None:
                                    current_node_id = current_node.parent.node_id
                                else:
                                    break
                            return f"unknown_{current_node_id}"  # å¦‚æœè¿½æº¯å¤±è´¥ï¼Œè¿”å›æ ‡è®°
                        
                        # æ„å»ºæ‰€æœ‰èŠ‚ç‚¹çš„å­—å…¸ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
                        all_nodes_dict = {}
                        for node in all_nodes:
                            all_nodes_dict[node.node_id] = node
                        
                        # ğŸ” è°ƒè¯•ï¼šç»Ÿè®¡æœ€åå±‚æ ·æœ¬çš„stepåˆ†å¸ƒ
                        step_distribution = {}
                        for sample in last_layer_samples:
                            step = sample["step"]
                            step_distribution[step] = step_distribution.get(step, 0) + 1
                        main_print(f"ğŸ” æœ€åå±‚æ ·æœ¬stepåˆ†å¸ƒ: {step_distribution}")
                        
                        # æŒ‰æœ€ååˆ†è£‚åˆ†æ”¯åˆ†ç»„
                        branch_groups = {}
                        unknown_count = 0
                        for sample in last_layer_samples:
                            branch_id = find_last_split_branch(sample, all_nodes_dict)
                            if branch_id.startswith("unknown_"):
                                unknown_count += 1
                            if branch_id not in branch_groups:
                                branch_groups[branch_id] = []
                            branch_groups[branch_id].append(sample)
                        
                        if unknown_count > 0:
                            main_print(f"âš ï¸  æ— æ³•è¿½æº¯çš„æ ·æœ¬æ•°: {unknown_count}")
                        
                        # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºå‰å‡ ä¸ªåˆ†æ”¯ç»„çš„ä¿¡æ¯
                        for i, (branch_id, group) in enumerate(branch_groups.items()):
                            if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                                main_print(f"ğŸ” åˆ†æ”¯ {branch_id}: {len(group)}ä¸ªæ ·æœ¬")
                        
                        main_print(f"ğŸ” æœ€ååˆ†è£‚åˆ†æ”¯æ•°: {len(branch_groups)}")
                        
                        # å¯¹æ¯ä¸ªåˆ†æ”¯ç»„å†…æŒ‰advantageæ’åºï¼Œä¿ç•™å‰width_pruning_ratio
                        filtered_last_layer = []
                        total_pruned = 0
                        for branch_id, group in branch_groups.items():
                            group_size = len(group)
                            keep_count = max(1, int(group_size * width_pruning_ratio))  # è‡³å°‘ä¿ç•™1ä¸ª
                            
                            # æŒ‰advantageé™åºæ’åºï¼ˆæœ€å¥½çš„åœ¨å‰é¢ï¼‰
                            sorted_group = sorted(group, key=lambda x: x["advantage"].item() if isinstance(x["advantage"], torch.Tensor) else float(x["advantage"]), reverse=True)
                            
                            kept_samples = sorted_group[:keep_count]
                            filtered_last_layer.extend(kept_samples)
                            total_pruned += (group_size - keep_count)
                        
                        # ç»„åˆå…¶ä»–å±‚çš„æ ·æœ¬å’Œç­›é€‰åçš„æœ€åä¸€å±‚æ ·æœ¬
                        training_samples = other_samples + filtered_last_layer
                        main_print(f"   æŒ‰æœ€ååˆ†è£‚åˆ†æ”¯ç»„è£å‰ª: {len(branch_groups)}ä¸ªåˆ†æ”¯ç»„")
                        main_print(f"   å¹³å‡æ¯åˆ†æ”¯æ ·æœ¬æ•°: {len(last_layer_samples)/len(branch_groups):.1f}")
                        
                    elif mode == 2:
                        main_print(f"ğŸŒ³ å®½åº¦è£å‰ªæ¨¡å¼2: åœ¨æœ€åå±‚è½¬ç§»ä¸­ä¿ç•™æœ€å¥½å’Œæœ€åå„{width_pruning_ratio/2*100:.0f}%æ ·æœ¬")
                        
                        sorted_last_layer = sorted(last_layer_samples, key=lambda x: x["advantage"].item() if isinstance(x["advantage"], torch.Tensor) else float(x["advantage"]), reverse=True)
                        
                        total_last_layer = len(sorted_last_layer)
                        keep_count = max(2, int(total_last_layer * width_pruning_ratio))  
                        
                        # è®¡ç®—æœ€å¥½å’Œæœ€åå„ä¿ç•™å¤šå°‘
                        best_count = keep_count // 2
                        worst_count = keep_count - best_count
                        
                        # ä¿ç•™æœ€å¥½çš„å’Œæœ€åçš„
                        best_samples = sorted_last_layer[:best_count]
                        worst_samples = sorted_last_layer[-worst_count:] if worst_count > 0 else []
                        
                        filtered_last_layer = best_samples + worst_samples
                        total_pruned = total_last_layer - len(filtered_last_layer)
                        
                        # ç»„åˆå…¶ä»–å±‚çš„æ ·æœ¬å’Œç­›é€‰åçš„æœ€åä¸€å±‚æ ·æœ¬
                        training_samples = other_samples + filtered_last_layer
                        main_print(f"   ä¿ç•™æœ€åå±‚æœ€å¥½æ ·æœ¬: {best_count}ä¸ª, æœ€åæ ·æœ¬: {worst_count}ä¸ª")
                        
                    else:
                        main_print(f"Warning: æœªçŸ¥çš„å®½åº¦è£å‰ªæ¨¡å¼: {mode}, è·³è¿‡å®½åº¦è£å‰ª")
                        total_pruned = 0
                
                if mode in [1, 2]:
                    main_print(f"   æ·±åº¦è£å‰ªåæ ·æœ¬æ•°: {samples_after_depth_pruning}")
                    main_print(f"   å®½åº¦è£å‰ªåæ ·æœ¬æ•°: {len(training_samples)}")
                    main_print(f"   å®½åº¦è£å‰ªæ ·æœ¬æ•°: {total_pruned}")
                    main_print(f"   å®½åº¦è£å‰ªæ¯”ä¾‹: {total_pruned/len(last_layer_samples)*100:.1f}% (ä»…é’ˆå¯¹æœ€åå±‚è½¬ç§»)")
                    main_print(f"   æ€»è£å‰ªæ¯”ä¾‹: {(original_sample_count-len(training_samples))/original_sample_count*100:.1f}%")
            
        except Exception as e:
            main_print(f"Warning: å®½åº¦è£å‰ªæ‰§è¡Œå¤±è´¥: {e}")
    else:
        if should_prune:
            main_print(f"æ— å®½åº¦è£å‰ªï¼Œè®­ç»ƒæ ·æœ¬æ•°: {len(training_samples)}")
        else:
            main_print(f"ğŸ• å½“å‰æ­¥éª¤è¶…å‡ºè£å‰ªèŒƒå›´ï¼Œè·³è¿‡æ‰€æœ‰è£å‰ªï¼Œè®­ç»ƒæ ·æœ¬æ•°: {len(training_samples)}")
    
    # éšæœºæ‰“ä¹±è®­ç»ƒæ ·æœ¬ï¼ˆç±»ä¼¼åŸå§‹ GRPOï¼‰
    import random
    random.shuffle(training_samples)
    
    # ğŸŒŸ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ä¸åŸå§‹GRPOç›¸åŒçš„æ¢¯åº¦ç´¯ç§¯é€»è¾‘
    total_log_loss = 0.0
    grad_norm = None
    
    # ğŸ•’ æ—¶é—´è®°å½•ï¼šæŸå¤±è®¡ç®—å’Œåå‘ä¼ æ’­å¼€å§‹
    backward_start_time = time.time()
    
    # å‡†å¤‡å›¾åƒIDï¼ˆå¯¹æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒçš„é…ç½®ï¼‰
    latent_h, latent_w = args.h // 8, args.w // 8  # VAEä¸‹é‡‡æ ·ç³»æ•°æ˜¯8
    image_ids = prepare_latent_image_ids(
        1,  # æ¯æ¬¡å¤„ç†ä¸€ä¸ªæ ·æœ¬
        latent_h // 2,  # packåçš„é«˜åº¦
        latent_w // 2,  # packåçš„å®½åº¦
        device, 
        torch.bfloat16
    )
    
    # è®¡ç®—æ¯æ£µæ ‘çš„å¹³å‡è½¬ç§»æ•°ç”¨äºlosså½’ä¸€åŒ–ï¼Œé¿å…batchå˜å¤§å¯¼è‡´æœ‰æ•ˆå­¦ä¹ ç‡ä¸‹é™
    num_trees = max(1, len(root_nodes))
    avg_samples_per_tree = max(1, len(training_samples) // num_trees)
    num_samples_per_step = avg_samples_per_tree
    for i, sample in enumerate(training_samples):
        # 4. ä¸ºå½“å‰æ ·æœ¬è®¡ç®—æ–°çš„å¯¹æ•°æ¦‚ç‡
        single_latent = sample["latent"]
        single_next_latent = sample["next_latent"]
        single_step = sample["step"]
        sample_batch_idx = sample["batch_idx"]  # ğŸ”§ è·å–æ‰¹æ¬¡ç´¢å¼•
        single_timestep = torch.tensor([int(sigma_schedule[single_step] * 1000)], 
                                     device=device, dtype=torch.long)
        
        # ğŸ”§ æ ¹æ®batch_idxæå–å¯¹åº”çš„å‚æ•°ï¼Œç¡®ä¿ç»´åº¦åŒ¹é…
        sample_encoder_hidden_states = encoder_hidden_states[sample_batch_idx:sample_batch_idx+1]
        sample_pooled_prompt_embeds = pooled_prompt_embeds[sample_batch_idx:sample_batch_idx+1]
        sample_text_ids = text_ids[sample_batch_idx:sample_batch_idx+1]
        
        
        # 5. è®¡ç®—æ–°çš„å¯¹æ•°æ¦‚ç‡
        new_log_prob = grpo_one_step(
            args,
            single_latent,
            single_next_latent,
            sample_encoder_hidden_states,  # ç°åœ¨æ˜¯[1, text_seq_len, text_channels]
            sample_pooled_prompt_embeds,   # ç°åœ¨æ˜¯[1, ...]
            sample_text_ids,               # ç°åœ¨æ˜¯[1, ...]
            image_ids,                     # image_idsé€šå¸¸æ˜¯å…±äº«çš„
            transformer,
            single_timestep,
            single_step,
            sigma_schedule,
        )
        
        # 6. è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡å’Œ PPO clipped loss
        clip_range = args.clip_range
        adv_clip_max = args.adv_clip_max
        
        # è·å–advantageå¹¶ç¡®ä¿æ­£ç¡®çš„å½¢çŠ¶
        adv = sample["advantage"]
        if adv.dim() > 0:
            advantage = adv.item()  # è½¬ä¸ºæ ‡é‡
        else:
            advantage = adv.item()
        advantage = torch.tensor(advantage, device=device)
        
        # å¯¹ä¼˜åŠ¿è¿›è¡Œå‰ªåˆ‡
        clipped_advantage = torch.clamp(advantage, -adv_clip_max, adv_clip_max)
        
        # è®¡ç®—æ¯”ç‡
        old_log_prob = sample["log_prob"]
        ratio = torch.exp(new_log_prob - old_log_prob)
        
        # PPO clipped loss
        unclipped_loss = -clipped_advantage * ratio
        clipped_loss = -clipped_advantage * torch.clamp(
            ratio,
            1.0 - clip_range,
            1.0 + clip_range,
        )
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(ratio).any() or torch.isinf(ratio).any():
            print(f"Warning: ratio contains NaN or Inf at sample {i}")
            print(f"  new_log_prob: {new_log_prob}")
            print(f"  old_log_prob: {old_log_prob}")
            continue  # è·³è¿‡è¿™ä¸ªæ ·æœ¬
            
        if torch.isnan(clipped_advantage).any() or torch.isinf(clipped_advantage).any():
            print(f"Warning: advantage contains NaN or Inf at sample {i}")
            continue  # è·³è¿‡è¿™ä¸ªæ ·æœ¬
        
        # ğŸŒŸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ä¸åŸå§‹GRPOå®Œå…¨ç›¸åŒçš„lossè®¡ç®—
        # åŸå§‹GRPOä½¿ç”¨: loss = torch.mean(torch.maximum(unclipped_loss, clipped_loss)) / (args.gradient_accumulation_steps * train_timesteps)
        # è¿™é‡Œæˆ‘ä»¬ç”¨æ€»æ ·æœ¬æ•°æ›¿ä»£train_timesteps
        loss = torch.mean(torch.maximum(unclipped_loss, clipped_loss)) / (args.gradient_accumulation_steps * num_samples_per_step)
        
        # æ£€æŸ¥ loss æ˜¯å¦ä¸º NaN æˆ– Inf
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"Warning: loss is NaN or Inf at sample {i}, skipping")
            continue
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # ç´¯ç§¯ loss ç”¨äºæ—¥å¿—è®°å½•
        avg_loss = loss.detach().clone()
        dist.all_reduce(avg_loss, op=dist.ReduceOp.AVG)
        total_log_loss += avg_loss.item()
        
        # ğŸŒŸ å…³é”®ä¿®æ”¹ï¼šåªæœ‰å½“ç´¯ç§¯åˆ°gradient_accumulation_stepsæ—¶æ‰æ‰§è¡Œoptimizer.step()
        # è¿™ä¸åŸå§‹GRPOçš„é€»è¾‘å®Œå…¨ä¸€è‡´: if (i+1)%args.gradient_accumulation_steps==0:
        if (i+1) % args.gradient_accumulation_steps == 0:
            grad_norm = transformer.clip_grad_norm_(max_grad_norm)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            
        # è°ƒè¯•è¾“å‡ºï¼ˆæ¯éš”ä¸€å®šæ­¥æ•°ï¼‰
        if dist.get_rank() % 8 == 0 and i % 10 == 0:
            print(f"TreeGRPO training - sample {i}/{len(training_samples)}, ratio: {ratio.mean().item():.4f}, adv: {advantage.item():.4f}, loss: {loss.item():.4f}")
        
    # å¦‚æœæœ€åè¿˜æœ‰æœªå®Œæˆçš„æ¢¯åº¦ç´¯ç§¯ï¼Œæ‰§è¡Œæœ€åä¸€æ¬¡æ›´æ–°
    if len(training_samples) % args.gradient_accumulation_steps != 0:
        if grad_norm is None:  # å¦‚æœè¿˜æ²¡æœ‰æ‰§è¡Œè¿‡clip_grad_norm_
            grad_norm = transformer.clip_grad_norm_(max_grad_norm)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
    
    # å¦‚æœgrad_normè¿˜æ˜¯Noneï¼Œè¯´æ˜æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬
    if grad_norm is None:
        grad_norm = torch.tensor(0.0)
    
    # ğŸ•’ æ—¶é—´è®°å½•ï¼šæŸå¤±è®¡ç®—å’Œåå‘ä¼ æ’­ç»“æŸ
    backward_end_time = time.time()
    
    # ğŸ•’ è®¡ç®—å„ä¸ªç¯èŠ‚çš„æ—¶é—´
    sample_time = sample_end_time - sample_start_time
    reward_time = reward_end_time - reward_start_time
    backward_time = backward_end_time - backward_start_time
    total_step_time = backward_end_time - step_start_time
    
    # ğŸ•’ æ‰“å°æ—¶é—´ç»Ÿè®¡ä¿¡æ¯
    if dist.get_rank() == 0:
        main_print(f"â±ï¸  è®­ç»ƒæ—¶é—´ç»Ÿè®¡:")
        main_print(f"   é‡‡æ ·æ—¶é—´: {sample_time:.2f}s ({sample_time/total_step_time*100:.1f}%)")
        main_print(f"   å¥–åŠ±è®¡ç®—: {reward_time:.2f}s ({reward_time/total_step_time*100:.1f}%)")
        main_print(f"   æŸå¤±åå‘ä¼ æ’­: {backward_time:.2f}s ({backward_time/total_step_time*100:.1f}%)")
        main_print(f"   æ€»æ—¶é—´: {total_step_time:.2f}s")
    
    # ğŸŒŸ ä¿®å¤lossæ—¥å¿—è®°å½•ï¼šè¿”å›å¹³å‡lossè€Œéç´¯ç§¯lossï¼Œæ›´å‡†ç¡®åæ˜ è®­ç»ƒçŠ¶æ€
    # è®¡ç®—å®é™…å¤„ç†çš„æ ·æœ¬æ•°ï¼ˆæ’é™¤è·³è¿‡çš„NaNæ ·æœ¬ï¼‰
    effective_samples = len(training_samples)  # ç®€åŒ–ï¼šå‡è®¾å¤§éƒ¨åˆ†æ ·æœ¬éƒ½æ˜¯æœ‰æ•ˆçš„
    if effective_samples > 0:
        total_loss = total_log_loss / effective_samples  # å¹³å‡loss
    else:
        total_loss = 0.0
    
    # å‡†å¤‡è¿”å›çš„å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯
    reward_stats = {
        "mean": gathered_reward.mean().item() if gathered_reward.numel() > 0 else 0.0,
        "std": gathered_reward.std().item() if gathered_reward.numel() > 0 else 0.0,
        "min": gathered_reward.min().item() if gathered_reward.numel() > 0 else 0.0,
        "max": gathered_reward.max().item() if gathered_reward.numel() > 0 else 0.0,
    }
    
    return total_loss, grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm, reward_stats


def main(args):
    torch.backends.cuda.matmul.allow_tf32 = True

    local_rank = int(os.environ["LOCAL_RANK"])
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    dist.init_process_group("nccl")
    torch.cuda.set_device(local_rank)
    device = torch.cuda.current_device()
    initialize_sequence_parallel_state(args.sp_size)

    # If passed along, set the training seed now. On GPU...
    if args.seed is not None:
        set_seed(args.seed + rank)

    # ç”Ÿæˆå®éªŒåå¹¶ä¿å­˜åˆ°argsä¸­
    exp_name = generate_experiment_name(args)
    args._exp_name = exp_name  # ä¿å­˜å®éªŒååˆ°argsä¸­ï¼Œä¾›å…¶ä»–å‡½æ•°ä½¿ç”¨
    
    # ä¿å­˜å®éªŒé…ç½®
    save_experiment_config(args, exp_name, rank)
    
    # Handle the repository creation
    if rank <= 0 and args.output_dir is not None:
        os.makedirs(args.output_dir, exist_ok=True)
    
    # ä¸ºæ ‘å½¢ rollout åˆ›å»ºå®éªŒç‰¹å®šçš„ç›®å½•
    if rank <= 0:
        os.makedirs(f"images_branchgrpo/{exp_name}", exist_ok=True)
        os.makedirs(f"checkpoints/{exp_name}", exist_ok=True)
        os.makedirs(f"tmp/{exp_name}", exist_ok=True)

    # åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹
    preprocess_val = None
    if args.use_hpsv2:
        from hpsv2.src.open_clip import create_model_and_transforms, get_tokenizer
        from typing import Union
        import huggingface_hub
        from hpsv2.utils import root_path, hps_version_map
        def initialize_model():
            model_dict = {}
            model, preprocess_train, preprocess_val = create_model_and_transforms(
                'ViT-H-14',
                './hps_ckpt/open_clip_pytorch_model.bin',
                precision='amp',
                device=device,
                jit=False,
                force_quick_gelu=False,
                force_custom_text=False,
                force_patch_dropout=False,
                force_image_size=None,
                pretrained_image=False,
                image_mean=None,
                image_std=None,
                light_augmentation=True,
                aug_cfg={},
                output_dict=True,
                with_score_predictor=False,
                with_region_predictor=False
            )
            model_dict['model'] = model
            model_dict['preprocess_val'] = preprocess_val
            return model_dict
        model_dict = initialize_model()
        model = model_dict['model']
        preprocess_val = model_dict['preprocess_val']
        cp = "./hps_ckpt/HPS_v2.1_compressed.pt"

        checkpoint = torch.load(cp, map_location=f'cuda:{device}')
        model.load_state_dict(checkpoint['state_dict'])
        processor = get_tokenizer('ViT-H-14')
        reward_model = model.to(device)
        reward_model.eval()

    if args.use_pickscore:
        from transformers import AutoProcessor, AutoModel
        processor_name_or_path = "laion/CLIP-ViT-H-14-laion2B-s32B-b79K"
        model_pretrained_name_or_path = "yuvalkirstain/PickScore_v1"

        processor = AutoProcessor.from_pretrained(processor_name_or_path)
        reward_model = AutoModel.from_pretrained(model_pretrained_name_or_path).eval().to(device)

    main_print(f"--> loading model from {args.pretrained_model_name_or_path}")
    
    transformer = FluxTransformer2DModel.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder="transformer",
            torch_dtype = torch.float32
    )
    
    fsdp_kwargs, no_split_modules = get_dit_fsdp_kwargs(
        transformer,
        args.fsdp_sharding_startegy,
        False,
        args.use_cpu_offload,
        args.master_weight_type,
    )
    
    transformer = FSDP(transformer, **fsdp_kwargs,)

    if args.gradient_checkpointing:
        apply_fsdp_checkpointing(
            transformer, no_split_modules, args.selective_checkpointing
        )

    vae = AutoencoderKL.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="vae",
        torch_dtype = torch.bfloat16,
    ).to(device)

    main_print(
        f"--> Initializing FSDP with sharding strategy: {args.fsdp_sharding_startegy}"
    )
    main_print(f"--> model loaded")

    transformer.train()

    noise_scheduler = None

    params_to_optimize = transformer.parameters()
    params_to_optimize = list(filter(lambda p: p.requires_grad, params_to_optimize))

    optimizer = torch.optim.AdamW(
        params_to_optimize,
        lr=args.learning_rate,
        betas=(0.9, 0.999),
        weight_decay=args.weight_decay,
        eps=1e-8,
    )

    init_steps = 0
    main_print(f"optimizer: {optimizer}")

    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps,
        num_training_steps=1000000,
        num_cycles=args.lr_num_cycles,
        power=args.lr_power,
        last_epoch=init_steps - 1,
    )

    train_dataset = LatentDataset(args.data_json_path, args.num_latent_t, args.cfg)
    sampler = DistributedSampler(
            train_dataset, rank=rank, num_replicas=world_size, shuffle=True, seed=args.sampler_seed
        )

    train_dataloader = DataLoader(
        train_dataset,
        sampler=sampler,
        collate_fn=latent_collate_function,
        pin_memory=True,
        batch_size=args.train_batch_size,
        num_workers=args.dataloader_num_workers,
        drop_last=True,
    )

    if rank <= 0:
        project = "flux"
        wandb.init(project=project, name=exp_name, id=exp_name, config=args, resume="allow")

    # Train!
    total_batch_size = (
        args.train_batch_size
        * world_size
        * args.gradient_accumulation_steps
        / args.sp_size
        * args.train_sp_batch_size
    )
    main_print("***** Running Tree-based GRPO training *****")
    main_print(f"  Num examples = {len(train_dataset)}")
    main_print(f"  Dataloader size = {len(train_dataloader)}")
    main_print(f"  Resume training from step {init_steps}")
    main_print(f"  Instantaneous batch size per device = {args.train_batch_size}")
    main_print(
        f"  Total train batch size (w. data & sequence parallel, accumulation) = {total_batch_size}"
    )
    main_print(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    main_print(f"  Total optimization steps per epoch = {args.max_train_steps}")
    main_print(
        f"  Total training parameters per FSDP shard = {sum(p.numel() for p in transformer.parameters() if p.requires_grad) / 1e9} B"
    )
    main_print(f"  Master weight dtype: {transformer.parameters().__next__().dtype}")

    progress_bar = tqdm(
        range(0, 100000),
        initial=init_steps,
        desc="Steps",
        disable=local_rank > 0,
    )

    loader = sp_parallel_dataloader_wrapper(
        train_dataloader,
        device,
        args.train_batch_size,
        args.sp_size,
        args.train_sp_batch_size,
    )

    step_times = deque(maxlen=100)

    for epoch in range(1):
        if isinstance(sampler, DistributedSampler):
            sampler.set_epoch(epoch)

        for step in range(init_steps+1, args.max_train_steps+1):
            start_time = time.time()
            if step % args.checkpointing_steps == 0:
                checkpoint_dir = f"checkpoints/{exp_name}"
                save_checkpoint(transformer, rank, checkpoint_dir, step, epoch)
                dist.barrier()
            
            loss, grad_norm, reward_stats = train_one_step_tree(
                args,
                device, 
                transformer,
                vae,
                reward_model,
                processor,
                optimizer,
                lr_scheduler,
                loader,
                noise_scheduler,
                args.max_grad_norm,
                preprocess_val,
                current_step=step,
                max_steps=args.max_train_steps,
            )
    
            step_time = time.time() - start_time
            step_times.append(step_time)
            avg_step_time = sum(step_times) / len(step_times)
    
            progress_bar.set_postfix(
                {
                    "loss": f"{loss:.4f}",
                    "reward_mean": f"{reward_stats['mean']:.3f}",
                    "reward_std": f"{reward_stats['std']:.3f}",
                    "step_time": f"{step_time:.2f}s",
                    "grad_norm": grad_norm,
                }
            )
            progress_bar.update(1)
            if rank <= 0:
                wandb.log(
                    {
                        "train_loss": loss,
                        "learning_rate": lr_scheduler.get_last_lr()[0],
                        "step_time": step_time,
                        "avg_step_time": avg_step_time,
                        "grad_norm": grad_norm,
                        "reward_mean": reward_stats["mean"],
                        "reward_std": reward_stats["std"],
                        "reward_min": reward_stats["min"],
                        "reward_max": reward_stats["max"],
                    },
                    step=step,
                )

    if get_sequence_parallel_state():
        destroy_sequence_parallel_group()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # dataset & dataloader
    parser.add_argument("--data_json_path", type=str, required=True)
    parser.add_argument(
        "--dataloader_num_workers",
        type=int,
        default=10,
        help="Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.",
    )
    parser.add_argument(
        "--train_batch_size",
        type=int,
        default=16,
        help="Batch size (per device) for the training dataloader.",
    )
    parser.add_argument(
        "--num_latent_t",
        type=int,
        default=1,
        help="number of latent frames",
    )
    # text encoder & vae & diffusion model
    parser.add_argument("--pretrained_model_name_or_path", type=str)
    parser.add_argument("--dit_model_name_or_path", type=str, default=None)
    parser.add_argument("--vae_model_path", type=str, default=None, help="vae model.")
    parser.add_argument("--cache_dir", type=str, default="./cache_dir")

    # diffusion setting
    parser.add_argument("--ema_decay", type=float, default=0.995)
    parser.add_argument("--ema_start_step", type=int, default=0)
    parser.add_argument("--cfg", type=float, default=0.0)
    parser.add_argument(
        "--precondition_outputs",
        action="store_true",
        help="Whether to precondition the outputs of the model.",
    )

    # validation & logs
    parser.add_argument(
        "--seed", type=int, default=None, help="A seed for reproducible training."
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="The output directory where the model predictions and checkpoints will be written.",
    )
    parser.add_argument(
        "--checkpointing_steps",
        type=int,
        default=500,
        help=(
            "Save a checkpoint of the training state every X updates."
        ),
    )
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help=(
            "Whether training should be resumed from a previous checkpoint."
        ),
    )
    parser.add_argument(
        "--logging_dir",
        type=str,
        default="logs",
        help="TensorBoard log directory.",
    )

    # optimizer & scheduler & Training
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        help="Total number of training steps to perform.",
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help="Number of updates steps to accumulate before performing a backward/update pass.",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        help="Initial learning rate to use.",
    )
    parser.add_argument(
        "--lr_warmup_steps",
        type=int,
        default=10,
        help="Number of steps for the warmup in the lr scheduler.",
    )
    parser.add_argument(
        "--max_grad_norm", default=2.0, type=float, help="Max gradient norm."
    )
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        help="Whether or not to use gradient checkpointing to save memory.",
    )
    parser.add_argument("--selective_checkpointing", type=float, default=1.0)
    parser.add_argument(
        "--allow_tf32",
        action="store_true",
        help="Whether or not to allow TF32 on Ampere GPUs.",
    )
    parser.add_argument(
        "--mixed_precision",
        type=str,
        default=None,
        choices=["no", "fp16", "bf16"],
        help="Whether to use mixed precision.",
    )
    parser.add_argument(
        "--use_cpu_offload",
        action="store_true",
        help="Whether to use CPU offload for param & gradient & optimizer states.",
    )

    parser.add_argument("--sp_size", type=int, default=1, help="For sequence parallel")
    parser.add_argument(
        "--train_sp_batch_size",
        type=int,
        default=1,
        help="Batch size for sequence parallel training",
    )

    parser.add_argument("--fsdp_sharding_startegy", default="full")

    # lr_scheduler
    parser.add_argument(
        "--lr_scheduler",
        type=str,
        default="constant_with_warmup",
        help="The scheduler type to use.",
    )
    parser.add_argument(
        "--lr_num_cycles",
        type=int,
        default=1,
        help="Number of cycles in the learning rate scheduler.",
    )
    parser.add_argument(
        "--lr_power",
        type=float,
        default=1.0,
        help="Power factor of the polynomial scheduler.",
    )
    parser.add_argument(
        "--weight_decay", type=float, default=0.01, help="Weight decay to apply."
    )
    parser.add_argument(
        "--master_weight_type",
        type=str,
        default="fp32",
        help="Weight type to use - fp32 or bf16.",
    )

    #GRPO training parameters
    parser.add_argument("--h", type=int, default=None, help="video height")
    parser.add_argument("--w", type=int, default=None, help="video width")
    parser.add_argument("--t", type=int, default=None, help="video length")
    parser.add_argument("--sampling_steps", type=int, default=None, help="sampling steps")
    parser.add_argument("--eta", type=float, default=None, help="noise eta")
    parser.add_argument("--sampler_seed", type=int, default=None, help="seed of sampler")
    parser.add_argument("--loss_coef", type=float, default=1.0, help="the global loss should be divided by")
    parser.add_argument("--use_group", action="store_true", default=False, help="whether compute advantages for each prompt")
    parser.add_argument("--num_generations", type=int, default=16, help="num_generations per prompt")
    parser.add_argument("--use_hpsv2", action="store_true", default=False, help="whether use hpsv2 as reward model")
    parser.add_argument("--use_pickscore", action="store_true", default=False, help="whether use pickscore as reward model")
    parser.add_argument("--ignore_last", action="store_true", default=False, help="whether ignore last step of mdp")
    parser.add_argument("--init_same_noise", action="store_true", default=False, help="whether use the same noise within each prompt")
    parser.add_argument("--shift", type=float, default=1.0, help="shift for timestep scheduler")
    parser.add_argument("--timestep_fraction", type=float, default=1.0, help="timestep downsample ratio")
    parser.add_argument("--clip_range", type=float, default=1e-4, help="clip range for grpo")
    parser.add_argument("--adv_clip_max", type=float, default=5.0, help="clipping advantage")
    
    # Tree-specific parameters
    parser.add_argument("--tree_split_rounds", type=int, default=4, help="Number of split rounds for tree rollout")
    parser.add_argument("--tree_split_points", type=str, default=None, help="Comma-separated list of split points (e.g., '10,14,17,19'). If provided, overrides tree_split_rounds")
    parser.add_argument("--tree_split_noise_scale", type=float, default=0.3, help="Noise scale for tree splits")
    parser.add_argument("--tree_prob_weighted", action="store_true", default=False, help="Use child-edge log_prob softmax weighting for internal node rewards")
    parser.add_argument("--depth_pruning", type=str, default=None, help="Comma-separated list of depths to prune from training (e.g., '15,16,17,18'). Sampling and reward calculation remain unchanged.")
    parser.add_argument("--width_pruning_mode", type=int, default=0, choices=[0, 1, 2], help="Width pruning mode: 0=no width pruning, 1=keep best from each parent, 2=keep best and worst globally")
    parser.add_argument("--width_pruning_ratio", type=float, default=0.5, help="Ratio of samples to keep after width pruning (default: 0.5)")
    parser.add_argument("--pruning_step_ratio", type=float, default=1.0, help="Ratio of training steps where pruning is applied (0.5 = pruning in first 50%% of steps, default: 1.0 = always prune)")

    # Depth pruning sliding window
    parser.add_argument("--depth_pruning_slide", action="store_true", default=False, help="Enable sliding window for depth pruning")
    parser.add_argument("--depth_pruning_slide_interval", type=int, default=1, help="Slide the depth pruning window every N training steps")
    parser.add_argument("--depth_pruning_stop_depth", type=int, default=None, help="Optional: stop sliding when the shallowest depth reaches this value; if None, use last split parent depth")

    # æ ‘å½¢æ··åˆ ODE/SDEï¼šçª—å£å†…ä½¿ç”¨SDEï¼Œçª—å£å¤–ä½¿ç”¨ODEï¼›åˆ†è£‚æ­¥å§‹ç»ˆSDE
    parser.add_argument("--mix_ode_sde_tree", action="store_true", default=False, help="Enable mixed ODE/SDE on tree rollout: SDE inside a sliding window; ODE outside; split steps are always SDE")
    parser.add_argument("--mix_sde_window_size", type=int, default=4, help="Sliding window size (in steps) for SDE in tree rollout")

    args = parser.parse_args()
    main(args) 